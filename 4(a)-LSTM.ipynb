{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATASET_DIR = './kaggle-dataset/'\n",
    "GLOVE_DIR = './glove.6B/'\n",
    "SAVE_DIR = './'\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "X = pd.read_csv(os.path.join(DATASET_DIR, 'training_set_rel3.tsv'), sep='\\t', encoding='ISO-8859-1')\n",
    "y = X['domain1_score']\n",
    "X = X.dropna(axis=1)\n",
    "X = X.drop(columns=['rater1_domain1', 'rater2_domain1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   domain1_score  \n",
       "0              8  \n",
       "1              9  \n",
       "2              7  \n",
       "3             10  \n",
       "4              8  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum and Maximum Scores for each essay set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_scores = [-1, 2, 1, 0, 0, 0, 0, 0, 0]\n",
    "maximum_scores = [-1, 12, 6, 3, 3, 4, 4, 30, 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will preprocess all essays and convert them to feature vectors so that they can be fed into the RNN.\n",
    "\n",
    "These are all helper functions used to clean the essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def essay_to_wordlist(essay_v, remove_stopwords):\n",
    "    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n",
    "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n",
    "    words = essay_v.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return (words)\n",
    "\n",
    "def essay_to_sentences(essay_v, remove_stopwords):\n",
    "    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(essay_v.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    num_words = 0.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            num_words += 1\n",
    "            featureVec = np.add(featureVec,model[word])        \n",
    "    featureVec = np.divide(featureVec,num_words)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(essays, model, num_features):\n",
    "    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n",
    "    counter = 0\n",
    "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
    "    for essay in essays:\n",
    "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return essayFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a 2-Layer LSTM Model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PARIDHI\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
    "from keras.models import Sequential, load_model, model_from_config\n",
    "import keras.backend as K\n",
    "\n",
    "def get_model():\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True))\n",
    "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model on the dataset.\n",
    "\n",
    "We will use 5-Fold Cross Validation and measure the Quadratic Weighted Kappa for each fold.\n",
    "We will then calculate Average Kappa for all the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PARIDHI\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PARIDHI\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/120\n",
      "10380/10380 [==============================] - 10s 998us/step - loss: 64.3894 - mean_absolute_error: 4.3115\n",
      "Epoch 2/120\n",
      "10380/10380 [==============================] - 9s 843us/step - loss: 39.4969 - mean_absolute_error: 3.4606\n",
      "Epoch 3/120\n",
      "10380/10380 [==============================] - 8s 767us/step - loss: 33.8299 - mean_absolute_error: 3.3923\n",
      "Epoch 4/120\n",
      "10380/10380 [==============================] - 9s 844us/step - loss: 31.8572 - mean_absolute_error: 3.3592\n",
      "Epoch 5/120\n",
      "10380/10380 [==============================] - 8s 811us/step - loss: 30.4785 - mean_absolute_error: 3.3024\n",
      "Epoch 6/120\n",
      "10380/10380 [==============================] - 8s 791us/step - loss: 29.3624 - mean_absolute_error: 3.2441\n",
      "Epoch 7/120\n",
      "10380/10380 [==============================] - 8s 764us/step - loss: 27.6161 - mean_absolute_error: 3.0816\n",
      "Epoch 8/120\n",
      "10380/10380 [==============================] - 8s 790us/step - loss: 24.2419 - mean_absolute_error: 2.8469\n",
      "Epoch 9/120\n",
      "10380/10380 [==============================] - 9s 842us/step - loss: 20.9082 - mean_absolute_error: 2.6543\n",
      "Epoch 10/120\n",
      "10380/10380 [==============================] - 8s 785us/step - loss: 18.5511 - mean_absolute_error: 2.5041\n",
      "Epoch 11/120\n",
      "10380/10380 [==============================] - 8s 786us/step - loss: 17.8462 - mean_absolute_error: 2.42620s - los\n",
      "Epoch 12/120\n",
      "10380/10380 [==============================] - 8s 786us/step - loss: 16.3864 - mean_absolute_error: 2.3343\n",
      "Epoch 13/120\n",
      "10380/10380 [==============================] - 8s 805us/step - loss: 15.2030 - mean_absolute_error: 2.2340\n",
      "Epoch 14/120\n",
      "10380/10380 [==============================] - 8s 793us/step - loss: 15.2411 - mean_absolute_error: 2.2146\n",
      "Epoch 15/120\n",
      "10380/10380 [==============================] - 8s 791us/step - loss: 13.6093 - mean_absolute_error: 2.1077\n",
      "Epoch 16/120\n",
      "10380/10380 [==============================] - 9s 836us/step - loss: 13.5842 - mean_absolute_error: 2.0988\n",
      "Epoch 17/120\n",
      "10380/10380 [==============================] - 9s 834us/step - loss: 12.9178 - mean_absolute_error: 2.0378\n",
      "Epoch 18/120\n",
      "10380/10380 [==============================] - 8s 792us/step - loss: 12.6305 - mean_absolute_error: 2.0106\n",
      "Epoch 19/120\n",
      "10380/10380 [==============================] - 9s 844us/step - loss: 11.8875 - mean_absolute_error: 1.97691s - loss: 11.80 - ETA: 0s - loss: 11.82\n",
      "Epoch 20/120\n",
      "10380/10380 [==============================] - 9s 825us/step - loss: 11.5044 - mean_absolute_error: 1.9068\n",
      "Epoch 21/120\n",
      "10380/10380 [==============================] - 9s 859us/step - loss: 10.9427 - mean_absolute_error: 1.8771\n",
      "Epoch 22/120\n",
      "10380/10380 [==============================] - 8s 794us/step - loss: 11.1036 - mean_absolute_error: 1.8737\n",
      "Epoch 23/120\n",
      "10380/10380 [==============================] - 9s 842us/step - loss: 10.9257 - mean_absolute_error: 1.8634\n",
      "Epoch 24/120\n",
      "10380/10380 [==============================] - 8s 761us/step - loss: 10.5308 - mean_absolute_error: 1.8285\n",
      "Epoch 25/120\n",
      "10380/10380 [==============================] - 8s 812us/step - loss: 10.7262 - mean_absolute_error: 1.8175\n",
      "Epoch 26/120\n",
      "10380/10380 [==============================] - 8s 799us/step - loss: 9.4591 - mean_absolute_error: 1.7365\n",
      "Epoch 27/120\n",
      "10380/10380 [==============================] - 9s 844us/step - loss: 10.0027 - mean_absolute_error: 1.76208s - \n",
      "Epoch 28/120\n",
      "10380/10380 [==============================] - 8s 773us/step - loss: 9.6327 - mean_absolute_error: 1.7464 4s - loss: 10.2517 - m\n",
      "Epoch 29/120\n",
      "10380/10380 [==============================] - 9s 823us/step - loss: 9.5793 - mean_absolute_error: 1.7289\n",
      "Epoch 30/120\n",
      "10380/10380 [==============================] - 9s 823us/step - loss: 9.4235 - mean_absolute_error: 1.7033 0s - loss: 9.4755 - mean_absolute_error: 1.705 - ETA: 0s - loss: 9.4413 - mean_absolute_error: \n",
      "Epoch 31/120\n",
      "10380/10380 [==============================] - 8s 798us/step - loss: 9.6554 - mean_absolute_error: 1.7124\n",
      "Epoch 32/120\n",
      "10380/10380 [==============================] - 8s 801us/step - loss: 9.4477 - mean_absolute_error: 1.6945\n",
      "Epoch 33/120\n",
      "10380/10380 [==============================] - 8s 797us/step - loss: 9.2483 - mean_absolute_error: 1.6832 1s - loss: 9.4554\n",
      "Epoch 34/120\n",
      "10380/10380 [==============================] - 9s 847us/step - loss: 9.2108 - mean_absolute_error: 1.6695: 0s - loss: 9.1577 - mean_absolute_er\n",
      "Epoch 35/120\n",
      "10380/10380 [==============================] - 8s 787us/step - loss: 9.0887 - mean_absolute_error: 1.6596\n",
      "Epoch 36/120\n",
      "10380/10380 [==============================] - 8s 794us/step - loss: 9.0918 - mean_absolute_error: 1.6662\n",
      "Epoch 37/120\n",
      "10380/10380 [==============================] - 8s 770us/step - loss: 8.5874 - mean_absolute_error: 1.6389 7s \n",
      "Epoch 38/120\n",
      "10380/10380 [==============================] - 8s 808us/step - loss: 8.9866 - mean_absolute_error: 1.6347 5s  - ETA: 2s - l\n",
      "Epoch 39/120\n",
      "10380/10380 [==============================] - 8s 748us/step - loss: 9.1500 - mean_absolute_error: 1.6603 0s - loss: 9.0897 - mean_absolute_err\n",
      "Epoch 40/120\n",
      "10380/10380 [==============================] - 8s 791us/step - loss: 9.1564 - mean_absolute_error: 1.6467\n",
      "Epoch 41/120\n",
      "10380/10380 [==============================] - 8s 776us/step - loss: 8.8375 - mean_absolute_error: 1.6257\n",
      "Epoch 42/120\n",
      "10380/10380 [==============================] - 8s 789us/step - loss: 8.3802 - mean_absolute_error: 1.6079 1s - loss: 8.5331 - mean_abs\n",
      "Epoch 43/120\n",
      "10380/10380 [==============================] - 8s 745us/step - loss: 9.1448 - mean_absolute_error: 1.6356 2s - loss: 9.3796 - mean_absol - ETA: 0s - loss: 9.1731 - mean_absolute\n",
      "Epoch 44/120\n",
      "10380/10380 [==============================] - 8s 788us/step - loss: 8.7826 - mean_absolute_error: 1.6139\n",
      "Epoch 45/120\n",
      "10380/10380 [==============================] - 8s 779us/step - loss: 8.2087 - mean_absolute_error: 1.5931\n",
      "Epoch 46/120\n",
      "10380/10380 [==============================] - 8s 812us/step - loss: 8.4946 - mean_absolute_error: 1.5991\n",
      "Epoch 47/120\n",
      "10380/10380 [==============================] - 8s 785us/step - loss: 8.3283 - mean_absolute_error: 1.5862\n",
      "Epoch 48/120\n",
      "10380/10380 [==============================] - 8s 816us/step - loss: 8.1577 - mean_absolute_error: 1.5660\n",
      "Epoch 49/120\n",
      "10380/10380 [==============================] - 8s 784us/step - loss: 8.6250 - mean_absolute_error: 1.5900 0s - loss: 8.5926 - mean_absol\n",
      "Epoch 50/120\n",
      "10380/10380 [==============================] - 8s 800us/step - loss: 8.2779 - mean_absolute_error: 1.5761\n",
      "Epoch 51/120\n",
      "10380/10380 [==============================] - 8s 764us/step - loss: 8.1464 - mean_absolute_error: 1.5777\n",
      "Epoch 52/120\n",
      "10380/10380 [==============================] - 9s 823us/step - loss: 8.1673 - mean_absolute_error: 1.5624 3s  - ETA: 0s - loss: 8.2036 - mean_absolute_er\n",
      "Epoch 53/120\n",
      "10380/10380 [==============================] - 8s 758us/step - loss: 8.0647 - mean_absolute_error: 1.5608\n",
      "Epoch 54/120\n",
      "10380/10380 [==============================] - 8s 777us/step - loss: 8.0012 - mean_absolute_error: 1.5544 1s - loss: 8.0736 - mean_absolute - ETA: 0s - loss: 8.1398 - mean_absolute_error: \n",
      "Epoch 55/120\n",
      "10380/10380 [==============================] - 8s 757us/step - loss: 7.7375 - mean_absolute_error: 1.5423\n",
      "Epoch 56/120\n",
      "10380/10380 [==============================] - 9s 822us/step - loss: 8.1762 - mean_absolute_error: 1.5597\n",
      "Epoch 57/120\n",
      "10380/10380 [==============================] - 8s 741us/step - loss: 8.0491 - mean_absolute_error: 1.5518\n",
      "Epoch 58/120\n",
      "10380/10380 [==============================] - 8s 767us/step - loss: 8.0013 - mean_absolute_error: 1.5403\n",
      "Epoch 59/120\n",
      "10380/10380 [==============================] - 8s 739us/step - loss: 7.8356 - mean_absolute_error: 1.5303\n",
      "Epoch 60/120\n",
      "10380/10380 [==============================] - 8s 789us/step - loss: 8.0021 - mean_absolute_error: 1.5212\n",
      "Epoch 61/120\n",
      "10380/10380 [==============================] - 8s 741us/step - loss: 7.6083 - mean_absolute_error: 1.5293\n",
      "Epoch 62/120\n",
      "10380/10380 [==============================] - ETA: 0s - loss: 7.8193 - mean_absolute_error: 1.534 - 8s 774us/step - loss: 7.8118 - mean_absolute_error: 1.5334\n",
      "Epoch 63/120\n",
      "10380/10380 [==============================] - 8s 765us/step - loss: 7.7402 - mean_absolute_error: 1.5107\n",
      "Epoch 64/120\n",
      "10380/10380 [==============================] - 8s 791us/step - loss: 8.0607 - mean_absolute_error: 1.5432\n",
      "Epoch 65/120\n",
      "10380/10380 [==============================] - 8s 748us/step - loss: 8.0500 - mean_absolute_error: 1.5382\n",
      "Epoch 66/120\n",
      "10380/10380 [==============================] - 8s 769us/step - loss: 7.8994 - mean_absolute_error: 1.5298\n",
      "Epoch 67/120\n",
      "10380/10380 [==============================] - ETA: 0s - loss: 7.8223 - mean_absolute_error: 1.529 - 8s 788us/step - loss: 7.8209 - mean_absolute_error: 1.5298\n",
      "Epoch 68/120\n",
      "10380/10380 [==============================] - 8s 787us/step - loss: 7.7084 - mean_absolute_error: 1.5157\n",
      "Epoch 69/120\n",
      "10380/10380 [==============================] - 8s 751us/step - loss: 7.6641 - mean_absolute_error: 1.4962\n",
      "Epoch 70/120\n",
      "10380/10380 [==============================] - 8s 786us/step - loss: 7.8248 - mean_absolute_error: 1.5167\n",
      "Epoch 71/120\n",
      "10380/10380 [==============================] - 8s 777us/step - loss: 7.5137 - mean_absolute_error: 1.4954\n",
      "Epoch 72/120\n",
      "10380/10380 [==============================] - 8s 785us/step - loss: 7.3758 - mean_absolute_error: 1.4888\n",
      "Epoch 73/120\n",
      "10380/10380 [==============================] - 8s 752us/step - loss: 7.6579 - mean_absolute_error: 1.4969\n",
      "Epoch 74/120\n",
      "10380/10380 [==============================] - 8s 798us/step - loss: 7.3835 - mean_absolute_error: 1.4887\n",
      "Epoch 75/120\n",
      "10380/10380 [==============================] - 8s 777us/step - loss: 7.5267 - mean_absolute_error: 1.4935\n",
      "Epoch 76/120\n",
      "10380/10380 [==============================] - 8s 784us/step - loss: 7.3703 - mean_absolute_error: 1.4809\n",
      "Epoch 77/120\n",
      "10380/10380 [==============================] - 8s 797us/step - loss: 7.5402 - mean_absolute_error: 1.4908\n",
      "Epoch 78/120\n",
      "10380/10380 [==============================] - 9s 838us/step - loss: 7.1667 - mean_absolute_error: 1.4706\n",
      "Epoch 79/120\n",
      "10380/10380 [==============================] - 8s 755us/step - loss: 7.5819 - mean_absolute_error: 1.4838\n",
      "Epoch 80/120\n",
      "10380/10380 [==============================] - 8s 793us/step - loss: 7.0302 - mean_absolute_error: 1.4585\n",
      "Epoch 81/120\n",
      "10380/10380 [==============================] - 8s 767us/step - loss: 7.1135 - mean_absolute_error: 1.4573\n",
      "Epoch 82/120\n",
      "10380/10380 [==============================] - 9s 830us/step - loss: 7.1994 - mean_absolute_error: 1.4616\n",
      "Epoch 83/120\n",
      "10380/10380 [==============================] - 8s 772us/step - loss: 7.3564 - mean_absolute_error: 1.4775\n",
      "Epoch 84/120\n",
      "10380/10380 [==============================] - 9s 827us/step - loss: 7.3133 - mean_absolute_error: 1.4730\n",
      "Epoch 85/120\n",
      "10380/10380 [==============================] - 8s 811us/step - loss: 7.5660 - mean_absolute_error: 1.4720\n",
      "Epoch 86/120\n",
      "10380/10380 [==============================] - 8s 801us/step - loss: 7.3560 - mean_absolute_error: 1.4747\n",
      "Epoch 87/120\n",
      "10380/10380 [==============================] - 8s 808us/step - loss: 6.9199 - mean_absolute_error: 1.4289 1s - loss: 6.6404\n",
      "Epoch 88/120\n",
      "10380/10380 [==============================] - 8s 774us/step - loss: 7.1408 - mean_absolute_error: 1.4499\n",
      "Epoch 89/120\n",
      "10380/10380 [==============================] - 9s 833us/step - loss: 7.7920 - mean_absolute_error: 1.4858 2s - loss: 7.8199 - m - ETA: 1s - loss: 8.0562 - mean_absolute_error - ETA: 1s - loss: 8.0980 - mean_abs\n",
      "Epoch 90/120\n",
      "10380/10380 [==============================] - 8s 780us/step - loss: 6.9877 - mean_absolute_error: 1.4351\n",
      "Epoch 91/120\n",
      "10380/10380 [==============================] - 8s 791us/step - loss: 7.1064 - mean_absolute_error: 1.4392\n",
      "Epoch 92/120\n",
      "10380/10380 [==============================] - 8s 788us/step - loss: 7.2698 - mean_absolute_error: 1.4533\n",
      "Epoch 93/120\n",
      "10380/10380 [==============================] - 9s 857us/step - loss: 7.4279 - mean_absolute_error: 1.4605 5s - loss: 7.5969 - mean_absolute_error: 1.471 - ETA: 5s - loss: 7\n",
      "Epoch 94/120\n",
      "10380/10380 [==============================] - 8s 789us/step - loss: 7.1144 - mean_absolute_error: 1.4491 1s - loss: 6.9930 - mean_abso\n",
      "Epoch 95/120\n",
      "10380/10380 [==============================] - 8s 795us/step - loss: 7.4748 - mean_absolute_error: 1.4638\n",
      "Epoch 96/120\n",
      "10380/10380 [==============================] - 8s 767us/step - loss: 6.8196 - mean_absolute_error: 1.4215\n",
      "Epoch 97/120\n",
      "10380/10380 [==============================] - 8s 783us/step - loss: 7.2149 - mean_absolute_error: 1.4539\n",
      "Epoch 98/120\n",
      "10380/10380 [==============================] - 8s 760us/step - loss: 6.6273 - mean_absolute_error: 1.4109\n",
      "Epoch 99/120\n",
      "10380/10380 [==============================] - 8s 813us/step - loss: 6.8195 - mean_absolute_error: 1.4210\n",
      "Epoch 100/120\n",
      "10380/10380 [==============================] - 8s 763us/step - loss: 6.8996 - mean_absolute_error: 1.4169\n",
      "Epoch 101/120\n",
      "10380/10380 [==============================] - 8s 775us/step - loss: 7.1352 - mean_absolute_error: 1.4516\n",
      "Epoch 102/120\n",
      "10380/10380 [==============================] - 8s 738us/step - loss: 7.2121 - mean_absolute_error: 1.4484\n",
      "Epoch 103/120\n",
      "10380/10380 [==============================] - 8s 786us/step - loss: 6.9140 - mean_absolute_error: 1.4361\n",
      "Epoch 104/120\n",
      "10380/10380 [==============================] - 8s 765us/step - loss: 6.9857 - mean_absolute_error: 1.4361\n",
      "Epoch 105/120\n",
      "10380/10380 [==============================] - 8s 801us/step - loss: 6.9398 - mean_absolute_error: 1.4326\n",
      "Epoch 106/120\n",
      "10380/10380 [==============================] - 8s 763us/step - loss: 7.3025 - mean_absolute_error: 1.4481 0s - loss: 7.1904 - mean_absolute_\n",
      "Epoch 107/120\n",
      "10380/10380 [==============================] - 8s 798us/step - loss: 6.9567 - mean_absolute_error: 1.4253\n",
      "Epoch 108/120\n",
      "10380/10380 [==============================] - 8s 750us/step - loss: 6.6612 - mean_absolute_error: 1.4124\n",
      "Epoch 109/120\n",
      "10380/10380 [==============================] - 8s 796us/step - loss: 7.2314 - mean_absolute_error: 1.4399\n",
      "Epoch 110/120\n",
      "10380/10380 [==============================] - 8s 751us/step - loss: 7.0464 - mean_absolute_error: 1.4446 1s - loss: 7.0645 - me\n",
      "Epoch 111/120\n",
      "10380/10380 [==============================] - 9s 830us/step - loss: 7.2273 - mean_absolute_error: 1.4518 4s - loss: 7.2793 - mean_\n",
      "Epoch 112/120\n",
      "10380/10380 [==============================] - 8s 757us/step - loss: 7.0013 - mean_absolute_error: 1.4413\n",
      "Epoch 113/120\n",
      "10380/10380 [==============================] - 8s 796us/step - loss: 7.0731 - mean_absolute_error: 1.4381 0s - loss: 7.0358 - mean_absolute_e\n",
      "Epoch 114/120\n",
      "10380/10380 [==============================] - 8s 734us/step - loss: 6.9706 - mean_absolute_error: 1.4368\n",
      "Epoch 115/120\n",
      "10380/10380 [==============================] - 9s 834us/step - loss: 6.6992 - mean_absolute_error: 1.4128 4s - loss: 7.0691 - mean_absolute_error: \n",
      "Epoch 116/120\n",
      "10380/10380 [==============================] - 8s 740us/step - loss: 6.9120 - mean_absolute_error: 1.4223\n",
      "Epoch 117/120\n",
      "10380/10380 [==============================] - 8s 783us/step - loss: 6.7948 - mean_absolute_error: 1.4232\n",
      "Epoch 118/120\n",
      "10380/10380 [==============================] - 8s 734us/step - loss: 7.1530 - mean_absolute_error: 1.4453\n",
      "Epoch 119/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10380/10380 [==============================] - 8s 796us/step - loss: 6.8809 - mean_absolute_error: 1.4263 7s - loss: 2.6649 - mea - ETA: 6s - loss: 5.3058 - mean_absolute_ - ETA: 5s - loss: 5.7706 - mean_absolute_er - ETA: 5s - loss: 5.7510 - mean_abs\n",
      "Epoch 120/120\n",
      "10380/10380 [==============================] - 7s 701us/step - loss: 6.9872 - mean_absolute_error: 1.4252\n",
      "Kappa Score: 0.9620323017556787\n",
      "\n",
      "--------Fold 2--------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/120\n",
      "10381/10381 [==============================] - 11s 1ms/step - loss: 64.5343 - mean_absolute_error: 4.3550\n",
      "Epoch 2/120\n",
      "10381/10381 [==============================] - 9s 822us/step - loss: 38.4065 - mean_absolute_error: 3.4703\n",
      "Epoch 3/120\n",
      "10381/10381 [==============================] - 8s 745us/step - loss: 32.4920 - mean_absolute_error: 3.4197\n",
      "Epoch 4/120\n",
      "10381/10381 [==============================] - 8s 773us/step - loss: 29.2185 - mean_absolute_error: 3.3315\n",
      "Epoch 5/120\n",
      "10381/10381 [==============================] - 8s 754us/step - loss: 28.0956 - mean_absolute_error: 3.23870s - loss: 28.05\n",
      "Epoch 6/120\n",
      "10381/10381 [==============================] - 9s 849us/step - loss: 26.7754 - mean_absolute_error: 3.1480\n",
      "Epoch 7/120\n",
      "10381/10381 [==============================] - 8s 764us/step - loss: 25.7374 - mean_absolute_error: 3.05461s - l\n",
      "Epoch 8/120\n",
      "10381/10381 [==============================] - 8s 785us/step - loss: 24.2990 - mean_absolute_error: 2.9143\n",
      "Epoch 9/120\n",
      "10381/10381 [==============================] - 8s 791us/step - loss: 23.3227 - mean_absolute_error: 2.8284\n",
      "Epoch 10/120\n",
      "10381/10381 [==============================] - 8s 807us/step - loss: 21.8812 - mean_absolute_error: 2.7326\n",
      "Epoch 11/120\n",
      "10381/10381 [==============================] - 8s 798us/step - loss: 20.7894 - mean_absolute_error: 2.6436\n",
      "Epoch 12/120\n",
      "10381/10381 [==============================] - 8s 782us/step - loss: 18.8329 - mean_absolute_error: 2.5224\n",
      "Epoch 13/120\n",
      "10381/10381 [==============================] - 9s 829us/step - loss: 17.0622 - mean_absolute_error: 2.4142\n",
      "Epoch 14/120\n",
      "10381/10381 [==============================] - 8s 782us/step - loss: 15.6219 - mean_absolute_error: 2.2868\n",
      "Epoch 15/120\n",
      "10381/10381 [==============================] - 8s 775us/step - loss: 15.6017 - mean_absolute_error: 2.2511\n",
      "Epoch 16/120\n",
      "10381/10381 [==============================] - 8s 778us/step - loss: 14.2969 - mean_absolute_error: 2.1648\n",
      "Epoch 17/120\n",
      "10381/10381 [==============================] - 9s 840us/step - loss: 13.6714 - mean_absolute_error: 2.1052\n",
      "Epoch 18/120\n",
      "10381/10381 [==============================] - 8s 761us/step - loss: 12.9561 - mean_absolute_error: 2.0597\n",
      "Epoch 19/120\n",
      "10381/10381 [==============================] - 9s 834us/step - loss: 12.5599 - mean_absolute_error: 2.0290\n",
      "Epoch 20/120\n",
      "10381/10381 [==============================] - 8s 775us/step - loss: 11.3709 - mean_absolute_error: 1.9371\n",
      "Epoch 21/120\n",
      "10381/10381 [==============================] - 8s 792us/step - loss: 11.1924 - mean_absolute_error: 1.9003\n",
      "Epoch 22/120\n",
      "10381/10381 [==============================] - 8s 753us/step - loss: 11.5161 - mean_absolute_error: 1.9072\n",
      "Epoch 23/120\n",
      "10381/10381 [==============================] - 8s 796us/step - loss: 10.7036 - mean_absolute_error: 1.8367\n",
      "Epoch 24/120\n",
      "10381/10381 [==============================] - 8s 768us/step - loss: 10.2236 - mean_absolute_error: 1.7975\n",
      "Epoch 25/120\n",
      "10381/10381 [==============================] - 8s 792us/step - loss: 10.2289 - mean_absolute_error: 1.7858\n",
      "Epoch 26/120\n",
      "10381/10381 [==============================] - 8s 794us/step - loss: 10.2593 - mean_absolute_error: 1.78241s\n",
      "Epoch 27/120\n",
      "10381/10381 [==============================] - 9s 865us/step - loss: 9.9130 - mean_absolute_error: 1.7593\n",
      "Epoch 28/120\n",
      "10381/10381 [==============================] - 8s 810us/step - loss: 9.9426 - mean_absolute_error: 1.7440\n",
      "Epoch 29/120\n",
      "10381/10381 [==============================] - 9s 823us/step - loss: 9.4854 - mean_absolute_error: 1.7183\n",
      "Epoch 30/120\n",
      "10381/10381 [==============================] - 8s 807us/step - loss: 9.2765 - mean_absolute_error: 1.7017\n",
      "Epoch 31/120\n",
      "10381/10381 [==============================] - 9s 880us/step - loss: 9.5167 - mean_absolute_error: 1.6986 1s - loss: 9.8947\n",
      "Epoch 32/120\n",
      "10381/10381 [==============================] - 9s 822us/step - loss: 9.1796 - mean_absolute_error: 1.6752 0s - loss: 9.1889 - mean_absolute_error: 1.676\n",
      "Epoch 33/120\n",
      "10381/10381 [==============================] - 9s 858us/step - loss: 9.0744 - mean_absolute_error: 1.6680\n",
      "Epoch 34/120\n",
      "10381/10381 [==============================] - 9s 838us/step - loss: 8.7263 - mean_absolute_error: 1.6473\n",
      "Epoch 35/120\n",
      "10381/10381 [==============================] - 9s 846us/step - loss: 8.6120 - mean_absolute_error: 1.6419\n",
      "Epoch 36/120\n",
      "10381/10381 [==============================] - 8s 815us/step - loss: 9.0509 - mean_absolute_error: 1.6490\n",
      "Epoch 37/120\n",
      "10381/10381 [==============================] - 8s 809us/step - loss: 8.8356 - mean_absolute_error: 1.6308\n",
      "Epoch 38/120\n",
      "10381/10381 [==============================] - 9s 880us/step - loss: 8.2944 - mean_absolute_error: 1.5969 2s - loss: 8.078\n",
      "Epoch 39/120\n",
      "10381/10381 [==============================] - 8s 792us/step - loss: 8.4932 - mean_absolute_error: 1.6037\n",
      "Epoch 40/120\n",
      "10381/10381 [==============================] - 9s 836us/step - loss: 8.3706 - mean_absolute_error: 1.5957\n",
      "Epoch 41/120\n",
      "10381/10381 [==============================] - 8s 805us/step - loss: 8.4553 - mean_absolute_error: 1.5959 0s - loss: 8.5182 - mean_absolute_error: 1. - ETA: 0s - loss: 8.4616 - mean_absolute_error: 1\n",
      "Epoch 42/120\n",
      "10381/10381 [==============================] - 9s 860us/step - loss: 8.5235 - mean_absolute_error: 1.5992\n",
      "Epoch 43/120\n",
      "10381/10381 [==============================] - 8s 794us/step - loss: 8.3939 - mean_absolute_error: 1.5908\n",
      "Epoch 44/120\n",
      "10381/10381 [==============================] - 9s 845us/step - loss: 8.1705 - mean_absolute_error: 1.5788\n",
      "Epoch 45/120\n",
      "10381/10381 [==============================] - 9s 864us/step - loss: 8.2077 - mean_absolute_error: 1.5740 2s - loss: 8.2160 - mean_ab - ETA: 1s - loss: 8.0849 \n",
      "Epoch 46/120\n",
      "10381/10381 [==============================] - 9s 821us/step - loss: 8.1884 - mean_absolute_error: 1.5660\n",
      "Epoch 47/120\n",
      "10381/10381 [==============================] - 9s 836us/step - loss: 8.1593 - mean_absolute_error: 1.5707\n",
      "Epoch 48/120\n",
      "10381/10381 [==============================] - 8s 815us/step - loss: 8.0057 - mean_absolute_error: 1.5515\n",
      "Epoch 49/120\n",
      "10381/10381 [==============================] - 9s 878us/step - loss: 8.5207 - mean_absolute_error: 1.5851 1s - loss: 8.4031 - mean_abs\n",
      "Epoch 50/120\n",
      "10381/10381 [==============================] - 8s 814us/step - loss: 7.7832 - mean_absolute_error: 1.5461\n",
      "Epoch 51/120\n",
      "10381/10381 [==============================] - 9s 827us/step - loss: 7.9004 - mean_absolute_error: 1.5510\n",
      "Epoch 52/120\n",
      "10381/10381 [==============================] - 9s 830us/step - loss: 8.1864 - mean_absolute_error: 1.5508\n",
      "Epoch 53/120\n",
      "10381/10381 [==============================] - 9s 855us/step - loss: 7.8422 - mean_absolute_error: 1.5423\n",
      "Epoch 54/120\n",
      "10381/10381 [==============================] - 9s 846us/step - loss: 7.5996 - mean_absolute_error: 1.5201 1s - loss: 7.6634\n",
      "Epoch 55/120\n",
      "10381/10381 [==============================] - 9s 832us/step - loss: 7.8161 - mean_absolute_error: 1.5230\n",
      "Epoch 56/120\n",
      "10381/10381 [==============================] - 8s 769us/step - loss: 7.8923 - mean_absolute_error: 1.5394\n",
      "Epoch 57/120\n",
      "10381/10381 [==============================] - 8s 778us/step - loss: 7.8497 - mean_absolute_error: 1.5426\n",
      "Epoch 58/120\n",
      "10381/10381 [==============================] - 8s 765us/step - loss: 7.8561 - mean_absolute_error: 1.5318\n",
      "Epoch 59/120\n",
      "10381/10381 [==============================] - 9s 820us/step - loss: 8.1290 - mean_absolute_error: 1.5430\n",
      "Epoch 60/120\n",
      "10381/10381 [==============================] - 8s 771us/step - loss: 7.6574 - mean_absolute_error: 1.5253\n",
      "Epoch 61/120\n",
      "10381/10381 [==============================] - 8s 795us/step - loss: 7.6404 - mean_absolute_error: 1.5096\n",
      "Epoch 62/120\n",
      "10381/10381 [==============================] - 8s 788us/step - loss: 7.8259 - mean_absolute_error: 1.5244\n",
      "Epoch 63/120\n",
      "10381/10381 [==============================] - 9s 824us/step - loss: 7.8291 - mean_absolute_error: 1.5151 1s - loss: 8.2341 \n",
      "Epoch 64/120\n",
      "10381/10381 [==============================] - 8s 785us/step - loss: 7.1554 - mean_absolute_error: 1.4896\n",
      "Epoch 65/120\n",
      "10381/10381 [==============================] - 8s 805us/step - loss: 7.5858 - mean_absolute_error: 1.5026\n",
      "Epoch 66/120\n",
      "10381/10381 [==============================] - 9s 819us/step - loss: 7.7955 - mean_absolute_error: 1.5031\n",
      "Epoch 67/120\n",
      "10381/10381 [==============================] - 8s 790us/step - loss: 7.8120 - mean_absolute_error: 1.5199\n",
      "Epoch 68/120\n",
      "10381/10381 [==============================] - 8s 795us/step - loss: 7.5235 - mean_absolute_error: 1.4986\n",
      "Epoch 69/120\n",
      "10381/10381 [==============================] - 8s 803us/step - loss: 8.0796 - mean_absolute_error: 1.5190\n",
      "Epoch 70/120\n",
      "10381/10381 [==============================] - 9s 869us/step - loss: 7.3019 - mean_absolute_error: 1.4884\n",
      "Epoch 71/120\n",
      "10381/10381 [==============================] - 8s 771us/step - loss: 7.4533 - mean_absolute_error: 1.4820\n",
      "Epoch 72/120\n",
      "10381/10381 [==============================] - 8s 809us/step - loss: 7.2837 - mean_absolute_error: 1.4810\n",
      "Epoch 73/120\n",
      "10381/10381 [==============================] - 8s 787us/step - loss: 7.5449 - mean_absolute_error: 1.4852\n",
      "Epoch 74/120\n",
      "10381/10381 [==============================] - 8s 817us/step - loss: 7.6379 - mean_absolute_error: 1.5027\n",
      "Epoch 75/120\n",
      "10381/10381 [==============================] - 8s 770us/step - loss: 7.7123 - mean_absolute_error: 1.5010\n",
      "Epoch 76/120\n",
      "10381/10381 [==============================] - ETA: 0s - loss: 7.1420 - mean_absolute_error: 1.4558- ETA: 2s - loss: 7.125 - 8s 813us/step - loss: 7.1340 - mean_absolute_error: 1.4549\n",
      "Epoch 77/120\n",
      "10381/10381 [==============================] - 8s 811us/step - loss: 7.4801 - mean_absolute_error: 1.4933 1s - loss: 7\n",
      "Epoch 78/120\n",
      "10381/10381 [==============================] - 9s 826us/step - loss: 7.2666 - mean_absolute_error: 1.4811\n",
      "Epoch 79/120\n",
      "10381/10381 [==============================] - 9s 837us/step - loss: 7.4409 - mean_absolute_error: 1.4754\n",
      "Epoch 80/120\n",
      "10381/10381 [==============================] - 9s 835us/step - loss: 7.1180 - mean_absolute_error: 1.4696\n",
      "Epoch 81/120\n",
      "10381/10381 [==============================] - 9s 845us/step - loss: 7.2924 - mean_absolute_error: 1.4683\n",
      "Epoch 82/120\n",
      "10381/10381 [==============================] - 8s 817us/step - loss: 7.2367 - mean_absolute_error: 1.4657\n",
      "Epoch 83/120\n",
      "10381/10381 [==============================] - 8s 803us/step - loss: 7.1097 - mean_absolute_error: 1.4610\n",
      "Epoch 84/120\n",
      "10381/10381 [==============================] - 9s 820us/step - loss: 7.4767 - mean_absolute_error: 1.4829\n",
      "Epoch 85/120\n",
      "10381/10381 [==============================] - 8s 800us/step - loss: 7.4581 - mean_absolute_error: 1.4774\n",
      "Epoch 86/120\n",
      "10381/10381 [==============================] - 8s 779us/step - loss: 7.5495 - mean_absolute_error: 1.4866\n",
      "Epoch 87/120\n",
      "10381/10381 [==============================] - 8s 811us/step - loss: 7.0647 - mean_absolute_error: 1.4574\n",
      "Epoch 88/120\n",
      "10381/10381 [==============================] - 8s 811us/step - loss: 7.2577 - mean_absolute_error: 1.4807\n",
      "Epoch 89/120\n",
      "10381/10381 [==============================] - 8s 817us/step - loss: 7.1991 - mean_absolute_error: 1.4645\n",
      "Epoch 90/120\n",
      "10381/10381 [==============================] - 8s 798us/step - loss: 7.0543 - mean_absolute_error: 1.4468\n",
      "Epoch 91/120\n",
      "10381/10381 [==============================] - 9s 857us/step - loss: 7.4232 - mean_absolute_error: 1.4685 2s - loss: 7.3421 - mean_absolute_er - ETA: 2s - loss\n",
      "Epoch 92/120\n",
      "10381/10381 [==============================] - 8s 772us/step - loss: 7.3932 - mean_absolute_error: 1.4806\n",
      "Epoch 93/120\n",
      "10381/10381 [==============================] - 8s 811us/step - loss: 7.2209 - mean_absolute_error: 1.4487\n",
      "Epoch 94/120\n",
      "10381/10381 [==============================] - 8s 760us/step - loss: 7.1954 - mean_absolute_error: 1.4644 3s - lo\n",
      "Epoch 95/120\n",
      "10381/10381 [==============================] - 9s 842us/step - loss: 7.1083 - mean_absolute_error: 1.4528 6s - loss: 7.7150 - mean_ - ETA: 5s - loss: 7.4130 - mean_absolute_er - ETA: 4s - loss: 7.3899 - mean_absolu\n",
      "Epoch 96/120\n",
      "10381/10381 [==============================] - 8s 777us/step - loss: 7.5100 - mean_absolute_error: 1.4797\n",
      "Epoch 97/120\n",
      "10381/10381 [==============================] - 8s 811us/step - loss: 7.4366 - mean_absolute_error: 1.4526\n",
      "Epoch 98/120\n",
      "10381/10381 [==============================] - 8s 802us/step - loss: 7.0499 - mean_absolute_error: 1.4478\n",
      "Epoch 99/120\n",
      "10381/10381 [==============================] - 9s 854us/step - loss: 7.1886 - mean_absolute_error: 1.4581\n",
      "Epoch 100/120\n",
      "10381/10381 [==============================] - 8s 771us/step - loss: 7.2390 - mean_absolute_error: 1.4563\n",
      "Epoch 101/120\n",
      "10381/10381 [==============================] - 8s 809us/step - loss: 6.8148 - mean_absolute_error: 1.4417\n",
      "Epoch 102/120\n",
      "10381/10381 [==============================] - 9s 825us/step - loss: 6.9946 - mean_absolute_error: 1.4405 0s - loss: 6.9753 - mean_absolute\n",
      "Epoch 103/120\n",
      "10381/10381 [==============================] - 9s 820us/step - loss: 7.0388 - mean_absolute_error: 1.4555\n",
      "Epoch 104/120\n",
      "10381/10381 [==============================] - 8s 802us/step - loss: 7.0468 - mean_absolute_error: 1.4410\n",
      "Epoch 105/120\n",
      "10381/10381 [==============================] - 8s 797us/step - loss: 6.8346 - mean_absolute_error: 1.4280 2s - loss: 7.0424 - - ETA: 0s - loss: 7.0176 - mean_absolute\n",
      "Epoch 106/120\n",
      "10381/10381 [==============================] - 9s 888us/step - loss: 6.6149 - mean_absolute_error: 1.4129\n",
      "Epoch 107/120\n",
      "10381/10381 [==============================] - 8s 816us/step - loss: 6.7402 - mean_absolute_error: 1.4362\n",
      "Epoch 108/120\n",
      "10381/10381 [==============================] - 9s 822us/step - loss: 7.1479 - mean_absolute_error: 1.4502\n",
      "Epoch 109/120\n",
      "10381/10381 [==============================] - 8s 801us/step - loss: 7.0251 - mean_absolute_error: 1.4489\n",
      "Epoch 110/120\n",
      "10381/10381 [==============================] - 8s 807us/step - loss: 6.9451 - mean_absolute_error: 1.4319\n",
      "Epoch 111/120\n",
      "10381/10381 [==============================] - 8s 768us/step - loss: 6.8167 - mean_absolute_error: 1.4183\n",
      "Epoch 112/120\n",
      "10381/10381 [==============================] - 8s 811us/step - loss: 7.0232 - mean_absolute_error: 1.4443\n",
      "Epoch 113/120\n",
      "10381/10381 [==============================] - 9s 849us/step - loss: 6.8635 - mean_absolute_error: 1.4365\n",
      "Epoch 114/120\n",
      "10381/10381 [==============================] - 9s 825us/step - loss: 6.9598 - mean_absolute_error: 1.4388 0s - loss: 6.6818 - mean_absolute_error\n",
      "Epoch 115/120\n",
      "10381/10381 [==============================] - 8s 792us/step - loss: 6.6007 - mean_absolute_error: 1.4119 \n",
      "Epoch 116/120\n",
      "10381/10381 [==============================] - 9s 875us/step - loss: 6.5348 - mean_absolute_error: 1.4110 4s - loss: 6.6052 - mean_abso - ETA: 3s - loss: 6.6220 - mean_absol - ETA: 2s - los\n",
      "Epoch 117/120\n",
      "10381/10381 [==============================] - 9s 883us/step - loss: 6.8031 - mean_absolute_error: 1.4251 4s - loss: 6. - ETA: 1s - loss: 7.0093 - mean_absolute_error: 1.443 - ETA: 1s - loss: 6.97\n",
      "Epoch 118/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10381/10381 [==============================] - 9s 828us/step - loss: 7.0114 - mean_absolute_error: 1.4468\n",
      "Epoch 119/120\n",
      "10381/10381 [==============================] - 9s 888us/step - loss: 6.5953 - mean_absolute_error: 1.4279\n",
      "Epoch 120/120\n",
      "10381/10381 [==============================] - 9s 850us/step - loss: 6.8354 - mean_absolute_error: 1.4248\n",
      "Kappa Score: 0.9671342305342847\n",
      "\n",
      "--------Fold 3--------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/120\n",
      "10381/10381 [==============================] - 11s 1ms/step - loss: 63.1771 - mean_absolute_error: 4.3103\n",
      "Epoch 2/120\n",
      "10381/10381 [==============================] - 8s 806us/step - loss: 38.9153 - mean_absolute_error: 3.4870\n",
      "Epoch 3/120\n",
      "10381/10381 [==============================] - 8s 804us/step - loss: 33.2916 - mean_absolute_error: 3.40730s - loss: 33.0365 - mean_\n",
      "Epoch 4/120\n",
      "10381/10381 [==============================] - 8s 778us/step - loss: 31.3993 - mean_absolute_error: 3.3935\n",
      "Epoch 5/120\n",
      "10381/10381 [==============================] - 8s 804us/step - loss: 29.7403 - mean_absolute_error: 3.33042s - l\n",
      "Epoch 6/120\n",
      "10381/10381 [==============================] - 8s 790us/step - loss: 28.6071 - mean_absolute_error: 3.2388\n",
      "Epoch 7/120\n",
      "10381/10381 [==============================] - 8s 766us/step - loss: 27.0358 - mean_absolute_error: 3.1259\n",
      "Epoch 8/120\n",
      "10381/10381 [==============================] - 8s 758us/step - loss: 26.2414 - mean_absolute_error: 2.9774\n",
      "Epoch 9/120\n",
      "10381/10381 [==============================] - 8s 770us/step - loss: 23.2936 - mean_absolute_error: 2.8054\n",
      "Epoch 10/120\n",
      "10381/10381 [==============================] - 8s 784us/step - loss: 20.6146 - mean_absolute_error: 2.6314\n",
      "Epoch 11/120\n",
      "10381/10381 [==============================] - 8s 797us/step - loss: 18.7417 - mean_absolute_error: 2.4809\n",
      "Epoch 12/120\n",
      "10381/10381 [==============================] - 9s 842us/step - loss: 17.4158 - mean_absolute_error: 2.3821\n",
      "Epoch 13/120\n",
      "10381/10381 [==============================] - 9s 896us/step - loss: 16.5979 - mean_absolute_error: 2.3323\n",
      "Epoch 14/120\n",
      "10381/10381 [==============================] - 9s 829us/step - loss: 15.9595 - mean_absolute_error: 2.29211s -\n",
      "Epoch 15/120\n",
      "10381/10381 [==============================] - 9s 826us/step - loss: 14.4474 - mean_absolute_error: 2.1889\n",
      "Epoch 16/120\n",
      "10381/10381 [==============================] - 8s 774us/step - loss: 14.3817 - mean_absolute_error: 2.1770\n",
      "Epoch 17/120\n",
      "10381/10381 [==============================] - 9s 855us/step - loss: 13.9575 - mean_absolute_error: 2.1375\n",
      "Epoch 18/120\n",
      "10381/10381 [==============================] - 8s 759us/step - loss: 13.6959 - mean_absolute_error: 2.0886\n",
      "Epoch 19/120\n",
      "10381/10381 [==============================] - 8s 798us/step - loss: 13.0345 - mean_absolute_error: 2.0409\n",
      "Epoch 20/120\n",
      "10381/10381 [==============================] - 8s 786us/step - loss: 12.6972 - mean_absolute_error: 2.0091\n",
      "Epoch 21/120\n",
      "10381/10381 [==============================] - 8s 816us/step - loss: 11.9446 - mean_absolute_error: 1.9659\n",
      "Epoch 22/120\n",
      "10381/10381 [==============================] - 8s 763us/step - loss: 11.7342 - mean_absolute_error: 1.9314\n",
      "Epoch 23/120\n",
      "10381/10381 [==============================] - 8s 806us/step - loss: 11.5666 - mean_absolute_error: 1.9069\n",
      "Epoch 24/120\n",
      "10381/10381 [==============================] - 8s 788us/step - loss: 11.0971 - mean_absolute_error: 1.8674\n",
      "Epoch 25/120\n",
      "10381/10381 [==============================] - 8s 777us/step - loss: 10.5791 - mean_absolute_error: 1.8279\n",
      "Epoch 26/120\n",
      "10381/10381 [==============================] - 8s 761us/step - loss: 10.8281 - mean_absolute_error: 1.82900s - loss: 10.7452 -\n",
      "Epoch 27/120\n",
      "10381/10381 [==============================] - 9s 827us/step - loss: 10.4557 - mean_absolute_error: 1.81300s - loss: 10.5223 - mean_absolute_e\n",
      "Epoch 28/120\n",
      "10381/10381 [==============================] - 9s 824us/step - loss: 10.2455 - mean_absolute_error: 1.7823\n",
      "Epoch 29/120\n",
      "10381/10381 [==============================] - 8s 765us/step - loss: 10.4951 - mean_absolute_error: 1.7815\n",
      "Epoch 30/120\n",
      "10381/10381 [==============================] - 8s 780us/step - loss: 10.2622 - mean_absolute_error: 1.7567\n",
      "Epoch 31/120\n",
      "10381/10381 [==============================] - 8s 785us/step - loss: 10.1723 - mean_absolute_error: 1.7745\n",
      "Epoch 32/120\n",
      "10381/10381 [==============================] - 8s 811us/step - loss: 10.0009 - mean_absolute_error: 1.7557\n",
      "Epoch 33/120\n",
      "10381/10381 [==============================] - 8s 775us/step - loss: 9.8868 - mean_absolute_error: 1.7372 2s - loss: 10\n",
      "Epoch 34/120\n",
      "10381/10381 [==============================] - 8s 768us/step - loss: 10.0558 - mean_absolute_error: 1.7221\n",
      "Epoch 35/120\n",
      "10381/10381 [==============================] - 8s 803us/step - loss: 9.2934 - mean_absolute_error: 1.7018\n",
      "Epoch 36/120\n",
      "10381/10381 [==============================] - 8s 797us/step - loss: 9.4049 - mean_absolute_error: 1.6976\n",
      "Epoch 37/120\n",
      "10381/10381 [==============================] - 8s 783us/step - loss: 9.8804 - mean_absolute_error: 1.7112\n",
      "Epoch 38/120\n",
      "10381/10381 [==============================] - 8s 792us/step - loss: 9.3984 - mean_absolute_error: 1.6889\n",
      "Epoch 39/120\n",
      "10381/10381 [==============================] - 9s 848us/step - loss: 8.7631 - mean_absolute_error: 1.6608\n",
      "Epoch 40/120\n",
      "10381/10381 [==============================] - 9s 846us/step - loss: 8.6142 - mean_absolute_error: 1.6508\n",
      "Epoch 41/120\n",
      "10381/10381 [==============================] - 8s 805us/step - loss: 8.8747 - mean_absolute_error: 1.6488\n",
      "Epoch 42/120\n",
      "10381/10381 [==============================] - 9s 830us/step - loss: 9.2560 - mean_absolute_error: 1.6771\n",
      "Epoch 43/120\n",
      "10381/10381 [==============================] - 8s 743us/step - loss: 8.7715 - mean_absolute_error: 1.6449\n",
      "Epoch 44/120\n",
      "10381/10381 [==============================] - 8s 802us/step - loss: 8.8042 - mean_absolute_error: 1.6407\n",
      "Epoch 45/120\n",
      "10381/10381 [==============================] - 8s 770us/step - loss: 9.0246 - mean_absolute_error: 1.6496\n",
      "Epoch 46/120\n",
      "10381/10381 [==============================] - 9s 840us/step - loss: 8.9770 - mean_absolute_error: 1.6315\n",
      "Epoch 47/120\n",
      "10381/10381 [==============================] - 8s 746us/step - loss: 8.7124 - mean_absolute_error: 1.6209\n",
      "Epoch 48/120\n",
      "10381/10381 [==============================] - 8s 794us/step - loss: 8.6845 - mean_absolute_error: 1.6144\n",
      "Epoch 49/120\n",
      "10381/10381 [==============================] - 8s 761us/step - loss: 8.6964 - mean_absolute_error: 1.5994\n",
      "Epoch 50/120\n",
      "10381/10381 [==============================] - 9s 824us/step - loss: 8.5531 - mean_absolute_error: 1.6034\n",
      "Epoch 51/120\n",
      "10381/10381 [==============================] - 8s 757us/step - loss: 8.2412 - mean_absolute_error: 1.5871\n",
      "Epoch 52/120\n",
      "10381/10381 [==============================] - 8s 808us/step - loss: 8.2591 - mean_absolute_error: 1.5878\n",
      "Epoch 53/120\n",
      "10381/10381 [==============================] - 8s 802us/step - loss: 8.3409 - mean_absolute_error: 1.5902\n",
      "Epoch 54/120\n",
      "10381/10381 [==============================] - 8s 809us/step - loss: 8.3380 - mean_absolute_error: 1.5782\n",
      "Epoch 55/120\n",
      "10381/10381 [==============================] - 8s 762us/step - loss: 8.4366 - mean_absolute_error: 1.5758\n",
      "Epoch 56/120\n",
      "10381/10381 [==============================] - 8s 796us/step - loss: 8.3550 - mean_absolute_error: 1.5767\n",
      "Epoch 57/120\n",
      "10381/10381 [==============================] - 8s 762us/step - loss: 8.2206 - mean_absolute_error: 1.5741\n",
      "Epoch 58/120\n",
      "10381/10381 [==============================] - 8s 762us/step - loss: 8.1504 - mean_absolute_error: 1.5640\n",
      "Epoch 59/120\n",
      "10381/10381 [==============================] - 8s 739us/step - loss: 8.2303 - mean_absolute_error: 1.5682\n",
      "Epoch 60/120\n",
      "10381/10381 [==============================] - 8s 777us/step - loss: 7.8922 - mean_absolute_error: 1.5394\n",
      "Epoch 61/120\n",
      "10381/10381 [==============================] - 8s 762us/step - loss: 7.9605 - mean_absolute_error: 1.5281\n",
      "Epoch 62/120\n",
      "10381/10381 [==============================] - 8s 768us/step - loss: 8.5840 - mean_absolute_error: 1.5907\n",
      "Epoch 63/120\n",
      "10381/10381 [==============================] - 8s 735us/step - loss: 7.8700 - mean_absolute_error: 1.5270\n",
      "Epoch 64/120\n",
      "10381/10381 [==============================] - 8s 791us/step - loss: 8.1790 - mean_absolute_error: 1.5417\n",
      "Epoch 65/120\n",
      "10381/10381 [==============================] - 8s 756us/step - loss: 8.3059 - mean_absolute_error: 1.5567\n",
      "Epoch 66/120\n",
      "10381/10381 [==============================] - 8s 781us/step - loss: 7.9510 - mean_absolute_error: 1.5368\n",
      "Epoch 67/120\n",
      "10381/10381 [==============================] - 8s 730us/step - loss: 8.1019 - mean_absolute_error: 1.5433\n",
      "Epoch 68/120\n",
      "10381/10381 [==============================] - 8s 816us/step - loss: 8.1252 - mean_absolute_error: 1.5407\n",
      "Epoch 69/120\n",
      "10381/10381 [==============================] - 8s 745us/step - loss: 7.9721 - mean_absolute_error: 1.5249\n",
      "Epoch 70/120\n",
      "10381/10381 [==============================] - 8s 773us/step - loss: 7.9462 - mean_absolute_error: 1.5262\n",
      "Epoch 71/120\n",
      "10381/10381 [==============================] - 9s 820us/step - loss: 7.7479 - mean_absolute_error: 1.5271\n",
      "Epoch 72/120\n",
      "10381/10381 [==============================] - 9s 837us/step - loss: 8.0142 - mean_absolute_error: 1.5433 6s - loss: 7.7916 - mean_absolute_error: 1. - ETA: 5s - loss: \n",
      "Epoch 73/120\n",
      "10381/10381 [==============================] - 8s 818us/step - loss: 7.8976 - mean_absolute_error: 1.5198\n",
      "Epoch 74/120\n",
      "10381/10381 [==============================] - 9s 858us/step - loss: 7.7201 - mean_absolute_error: 1.5169 0s - loss: 7.7473 - mean_absolute_error: 1.51\n",
      "Epoch 75/120\n",
      "10381/10381 [==============================] - 9s 894us/step - loss: 7.7888 - mean_absolute_error: 1.5111\n",
      "Epoch 76/120\n",
      "10381/10381 [==============================] - 9s 875us/step - loss: 7.9445 - mean_absolute_error: 1.5168\n",
      "Epoch 77/120\n",
      "10381/10381 [==============================] - 9s 901us/step - loss: 7.6419 - mean_absolute_error: 1.4994 1s - loss: 7.5365 - mean_absolu\n",
      "Epoch 78/120\n",
      "10381/10381 [==============================] - 10s 999us/step - loss: 7.7733 - mean_absolute_error: 1.51362s - loss: 7\n",
      "Epoch 79/120\n",
      "10381/10381 [==============================] - 10s 968us/step - loss: 7.1353 - mean_absolute_error: 1.48342s - loss\n",
      "Epoch 80/120\n",
      "10381/10381 [==============================] - 8s 767us/step - loss: 7.1834 - mean_absolute_error: 1.4747 0s - loss: 7.1842 - mean_absolute_error\n",
      "Epoch 81/120\n",
      "10381/10381 [==============================] - 8s 727us/step - loss: 7.8698 - mean_absolute_error: 1.5118\n",
      "Epoch 82/120\n",
      "10381/10381 [==============================] - 9s 829us/step - loss: 7.3619 - mean_absolute_error: 1.4838\n",
      "Epoch 83/120\n",
      "10381/10381 [==============================] - 9s 883us/step - loss: 7.4624 - mean_absolute_error: 1.4991\n",
      "Epoch 84/120\n",
      "10381/10381 [==============================] - 9s 848us/step - loss: 7.8046 - mean_absolute_error: 1.4960\n",
      "Epoch 85/120\n",
      "10381/10381 [==============================] - 9s 906us/step - loss: 7.5276 - mean_absolute_error: 1.4868\n",
      "Epoch 86/120\n",
      "10381/10381 [==============================] - 10s 931us/step - loss: 7.7450 - mean_absolute_error: 1.49700s - loss: 7.8363 - mean_absolute_err\n",
      "Epoch 87/120\n",
      "10381/10381 [==============================] - 9s 892us/step - loss: 7.8133 - mean_absolute_error: 1.4972 5s - loss: 7.8165 - mean_a - ETA: 4s - loss: 7.8276 - mean - ETA: 2s - loss: 7.7842 - mean_absolute_error: 1.500 - ETA: 2s - loss: 7.7480 - mean_absolute_error: 1.4 - ETA: 2s - loss: 7.8271 - - ETA: 0s - loss: 7.9402 - mean_absolut\n",
      "Epoch 88/120\n",
      "10381/10381 [==============================] - 9s 880us/step - loss: 7.7082 - mean_absolute_error: 1.5045\n",
      "Epoch 89/120\n",
      "10381/10381 [==============================] - 9s 883us/step - loss: 7.5132 - mean_absolute_error: 1.4734  - ETA: 0s - loss: 7.5849 - mean_absolute_erro\n",
      "Epoch 90/120\n",
      "10381/10381 [==============================] - 8s 734us/step - loss: 7.4556 - mean_absolute_error: 1.4782 1s - loss: 7.4179 - mean_absolu\n",
      "Epoch 91/120\n",
      "10381/10381 [==============================] - 8s 723us/step - loss: 7.7849 - mean_absolute_error: 1.4999\n",
      "Epoch 92/120\n",
      "10381/10381 [==============================] - 8s 791us/step - loss: 7.6210 - mean_absolute_error: 1.4773\n",
      "Epoch 93/120\n",
      "10381/10381 [==============================] - 9s 846us/step - loss: 7.7467 - mean_absolute_error: 1.4852\n",
      "Epoch 94/120\n",
      "10381/10381 [==============================] - 10s 965us/step - loss: 7.3327 - mean_absolute_error: 1.4759\n",
      "Epoch 95/120\n",
      "10381/10381 [==============================] - 11s 1ms/step - loss: 7.2266 - mean_absolute_error: 1.4689: 1s - loss: 7.4853 - mean_absolute_error: - ETA: 1s - loss: 7.5057 - mean_a\n",
      "Epoch 96/120\n",
      "10381/10381 [==============================] - 9s 902us/step - loss: 7.3374 - mean_absolute_error: 1.4764\n",
      "Epoch 97/120\n",
      "10381/10381 [==============================] - 9s 850us/step - loss: 7.3325 - mean_absolute_error: 1.4723\n",
      "Epoch 98/120\n",
      "10381/10381 [==============================] - 9s 896us/step - loss: 7.0666 - mean_absolute_error: 1.4586 6s - loss: 6.7333 - mean_absolute_erro - ETA: \n",
      "Epoch 99/120\n",
      "10381/10381 [==============================] - 9s 836us/step - loss: 7.8509 - mean_absolute_error: 1.4835\n",
      "Epoch 100/120\n",
      "10381/10381 [==============================] - 9s 909us/step - loss: 7.1040 - mean_absolute_error: 1.4606 1s - loss: 7.2880 - mean_absol\n",
      "Epoch 101/120\n",
      "10381/10381 [==============================] - 9s 833us/step - loss: 7.2275 - mean_absolute_error: 1.4597\n",
      "Epoch 102/120\n",
      "10381/10381 [==============================] - 9s 826us/step - loss: 7.1786 - mean_absolute_error: 1.4530\n",
      "Epoch 103/120\n",
      "10381/10381 [==============================] - 9s 827us/step - loss: 7.0847 - mean_absolute_error: 1.4491 2s - loss: 7\n",
      "Epoch 104/120\n",
      "10381/10381 [==============================] - 9s 856us/step - loss: 7.4959 - mean_absolute_error: 1.4698\n",
      "Epoch 105/120\n",
      "10381/10381 [==============================] - 9s 847us/step - loss: 7.1807 - mean_absolute_error: 1.4466\n",
      "Epoch 106/120\n",
      "10381/10381 [==============================] - 9s 842us/step - loss: 7.3518 - mean_absolute_error: 1.4557\n",
      "Epoch 107/120\n",
      "10381/10381 [==============================] - 8s 811us/step - loss: 7.5116 - mean_absolute_error: 1.4738\n",
      "Epoch 108/120\n",
      "10381/10381 [==============================] - 9s 830us/step - loss: 7.3515 - mean_absolute_error: 1.4544\n",
      "Epoch 109/120\n",
      "10381/10381 [==============================] - 9s 869us/step - loss: 7.2117 - mean_absolute_error: 1.4631 2s - loss: 7.2696 - mean_absolute_ - ETA: 1s - loss: 7.4427\n",
      "Epoch 110/120\n",
      "10381/10381 [==============================] - 8s 817us/step - loss: 7.1630 - mean_absolute_error: 1.4602\n",
      "Epoch 111/120\n",
      "10381/10381 [==============================] - 8s 816us/step - loss: 7.3243 - mean_absolute_error: 1.4556\n",
      "Epoch 112/120\n",
      "10381/10381 [==============================] - 9s 820us/step - loss: 6.9815 - mean_absolute_error: 1.4344 2s - loss: 6.6666 - mean_absolute_error: 1.424 - ETA: 2s - - ETA: 0s - loss: 6.8126 - mean_absolute_error:\n",
      "Epoch 113/120\n",
      "10381/10381 [==============================] - 9s 829us/step - loss: 7.2396 - mean_absolute_error: 1.4617\n",
      "Epoch 114/120\n",
      "10381/10381 [==============================] - 8s 770us/step - loss: 7.3854 - mean_absolute_error: 1.4644\n",
      "Epoch 115/120\n",
      "10381/10381 [==============================] - 9s 824us/step - loss: 7.1043 - mean_absolute_error: 1.4411\n",
      "Epoch 116/120\n",
      "10381/10381 [==============================] - 9s 830us/step - loss: 6.9967 - mean_absolute_error: 1.4318\n",
      "Epoch 117/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10381/10381 [==============================] - 8s 808us/step - loss: 7.0363 - mean_absolute_error: 1.4364\n",
      "Epoch 118/120\n",
      "10381/10381 [==============================] - 8s 747us/step - loss: 7.1247 - mean_absolute_error: 1.4470\n",
      "Epoch 119/120\n",
      "10381/10381 [==============================] - 9s 835us/step - loss: 7.2328 - mean_absolute_error: 1.4510\n",
      "Epoch 120/120\n",
      "10381/10381 [==============================] - 8s 775us/step - loss: 7.4967 - mean_absolute_error: 1.4533\n",
      "Kappa Score: 0.9694664048286248\n",
      "\n",
      "--------Fold 4--------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/120\n",
      "10381/10381 [==============================] - 12s 1ms/step - loss: 61.5628 - mean_absolute_error: 4.2834\n",
      "Epoch 2/120\n",
      "10381/10381 [==============================] - 8s 807us/step - loss: 37.2015 - mean_absolute_error: 3.4575\n",
      "Epoch 3/120\n",
      "10381/10381 [==============================] - 9s 852us/step - loss: 31.7539 - mean_absolute_error: 3.3891\n",
      "Epoch 4/120\n",
      "10381/10381 [==============================] - 8s 794us/step - loss: 29.1809 - mean_absolute_error: 3.3305\n",
      "Epoch 5/120\n",
      "10381/10381 [==============================] - 8s 812us/step - loss: 27.7438 - mean_absolute_error: 3.2671\n",
      "Epoch 6/120\n",
      "10381/10381 [==============================] - 8s 804us/step - loss: 26.6851 - mean_absolute_error: 3.1969\n",
      "Epoch 7/120\n",
      "10381/10381 [==============================] - 9s 841us/step - loss: 24.6327 - mean_absolute_error: 3.0172\n",
      "Epoch 8/120\n",
      "10381/10381 [==============================] - 8s 790us/step - loss: 23.7678 - mean_absolute_error: 2.8635\n",
      "Epoch 9/120\n",
      "10381/10381 [==============================] - 9s 834us/step - loss: 22.0541 - mean_absolute_error: 2.7641\n",
      "Epoch 10/120\n",
      "10381/10381 [==============================] - 9s 832us/step - loss: 19.7937 - mean_absolute_error: 2.5864\n",
      "Epoch 11/120\n",
      "10381/10381 [==============================] - 9s 819us/step - loss: 17.3034 - mean_absolute_error: 2.4211\n",
      "Epoch 12/120\n",
      "10381/10381 [==============================] - 8s 778us/step - loss: 14.9517 - mean_absolute_error: 2.2638\n",
      "Epoch 13/120\n",
      "10381/10381 [==============================] - 9s 844us/step - loss: 14.5145 - mean_absolute_error: 2.22030s - loss: 14.6568 - mean_absolute\n",
      "Epoch 14/120\n",
      "10381/10381 [==============================] - 9s 820us/step - loss: 13.8632 - mean_absolute_error: 2.1464\n",
      "Epoch 15/120\n",
      "10381/10381 [==============================] - 9s 827us/step - loss: 13.0080 - mean_absolute_error: 2.1264\n",
      "Epoch 16/120\n",
      "10381/10381 [==============================] - 8s 809us/step - loss: 12.8267 - mean_absolute_error: 2.08521s - loss: 12.6061 - mean_absolute_e - ETA: 0s - loss: 12.6920 -\n",
      "Epoch 17/120\n",
      "10381/10381 [==============================] - 9s 835us/step - loss: 12.5269 - mean_absolute_error: 2.0472\n",
      "Epoch 18/120\n",
      "10381/10381 [==============================] - 8s 818us/step - loss: 11.7466 - mean_absolute_error: 1.9887\n",
      "Epoch 19/120\n",
      "10381/10381 [==============================] - 8s 796us/step - loss: 11.7728 - mean_absolute_error: 1.9834\n",
      "Epoch 20/120\n",
      "10381/10381 [==============================] - 9s 834us/step - loss: 11.1179 - mean_absolute_error: 1.9194\n",
      "Epoch 21/120\n",
      "10381/10381 [==============================] - 8s 805us/step - loss: 10.7620 - mean_absolute_error: 1.8851\n",
      "Epoch 22/120\n",
      "10381/10381 [==============================] - 9s 824us/step - loss: 11.0455 - mean_absolute_error: 1.8998\n",
      "Epoch 23/120\n",
      "10381/10381 [==============================] - 8s 812us/step - loss: 10.3605 - mean_absolute_error: 1.8510\n",
      "Epoch 24/120\n",
      "10381/10381 [==============================] - 9s 857us/step - loss: 10.3147 - mean_absolute_error: 1.8331\n",
      "Epoch 25/120\n",
      "10381/10381 [==============================] - 8s 796us/step - loss: 10.1662 - mean_absolute_error: 1.80552s \n",
      "Epoch 26/120\n",
      "10381/10381 [==============================] - 10s 949us/step - loss: 10.2307 - mean_absolute_error: 1.8084s - loss: 10.2556 - mean_absolute_err\n",
      "Epoch 27/120\n",
      "10381/10381 [==============================] - 10s 972us/step - loss: 9.8737 - mean_absolute_error: 1.7663\n",
      "Epoch 28/120\n",
      "10381/10381 [==============================] - 8s 757us/step - loss: 9.5798 - mean_absolute_error: 1.7543 4s - loss: 9.7828 - mean_abs - ETA: 2s - loss: 9.6009\n",
      "Epoch 29/120\n",
      "10381/10381 [==============================] - 7s 716us/step - loss: 9.4909 - mean_absolute_error: 1.7500 0s - loss: 9.5238 - mean_absolute_error: 1.\n",
      "Epoch 30/120\n",
      "10381/10381 [==============================] - 8s 741us/step - loss: 9.3161 - mean_absolute_error: 1.7226\n",
      "Epoch 31/120\n",
      "10381/10381 [==============================] - 8s 730us/step - loss: 9.0788 - mean_absolute_error: 1.7073\n",
      "Epoch 32/120\n",
      "10381/10381 [==============================] - 8s 749us/step - loss: 9.1313 - mean_absolute_error: 1.7091 3s - loss: 8.7300 - mean_absolute_error: 1. - ETA: 3s - l - ETA: 0s - loss: 8.9300 - mean_absolute_err\n",
      "Epoch 33/120\n",
      "10381/10381 [==============================] - 8s 747us/step - loss: 9.2395 - mean_absolute_error: 1.7150\n",
      "Epoch 34/120\n",
      "10381/10381 [==============================] - 8s 740us/step - loss: 8.9962 - mean_absolute_error: 1.6989\n",
      "Epoch 35/120\n",
      "10381/10381 [==============================] - 8s 759us/step - loss: 8.8229 - mean_absolute_error: 1.6774 3s - loss: 8.828 - ETA: 0s - loss: 8.8592 - mean_absolute_e\n",
      "Epoch 36/120\n",
      "10381/10381 [==============================] - 8s 726us/step - loss: 9.0163 - mean_absolute_error: 1.6736\n",
      "Epoch 37/120\n",
      "10381/10381 [==============================] - 7s 686us/step - loss: 8.4276 - mean_absolute_error: 1.6473\n",
      "Epoch 38/120\n",
      "10381/10381 [==============================] - 8s 801us/step - loss: 8.3212 - mean_absolute_error: 1.6396\n",
      "Epoch 39/120\n",
      "10381/10381 [==============================] - 9s 833us/step - loss: 8.5133 - mean_absolute_error: 1.6638\n",
      "Epoch 40/120\n",
      "10381/10381 [==============================] - 9s 834us/step - loss: 8.2923 - mean_absolute_error: 1.6392\n",
      "Epoch 41/120\n",
      "10381/10381 [==============================] - 8s 811us/step - loss: 8.4067 - mean_absolute_error: 1.6230\n",
      "Epoch 42/120\n",
      "10381/10381 [==============================] - 9s 840us/step - loss: 8.3386 - mean_absolute_error: 1.6201\n",
      "Epoch 43/120\n",
      "10381/10381 [==============================] - 9s 822us/step - loss: 8.3534 - mean_absolute_error: 1.6074\n",
      "Epoch 44/120\n",
      "10381/10381 [==============================] - 9s 820us/step - loss: 8.2678 - mean_absolute_error: 1.6189\n",
      "Epoch 45/120\n",
      "10381/10381 [==============================] - 9s 830us/step - loss: 8.1465 - mean_absolute_error: 1.5883 1s - loss: 8.1189 - mea\n",
      "Epoch 46/120\n",
      "10381/10381 [==============================] - 9s 833us/step - loss: 7.9129 - mean_absolute_error: 1.5760 0s - loss: 7.9167 - mean_absolute_err\n",
      "Epoch 47/120\n",
      "10381/10381 [==============================] - 9s 824us/step - loss: 7.9420 - mean_absolute_error: 1.5840 3s - loss: 7.7606 - mean_absolute_error: 1.5 -\n",
      "Epoch 48/120\n",
      "10381/10381 [==============================] - 8s 793us/step - loss: 8.2372 - mean_absolute_error: 1.6028\n",
      "Epoch 49/120\n",
      "10381/10381 [==============================] - 9s 855us/step - loss: 7.8599 - mean_absolute_error: 1.5649 2s - loss: 8.1585 - mean_absolute_error - ETA: 1s - loss: 8.1280 - mean_absolute_error: 1.59 - ETA: 1s - loss: 8.116\n",
      "Epoch 50/120\n",
      "10381/10381 [==============================] - 8s 794us/step - loss: 8.0762 - mean_absolute_error: 1.5884\n",
      "Epoch 51/120\n",
      "10381/10381 [==============================] - 9s 825us/step - loss: 7.8935 - mean_absolute_error: 1.5689\n",
      "Epoch 52/120\n",
      "10381/10381 [==============================] - 8s 777us/step - loss: 7.9954 - mean_absolute_error: 1.5731\n",
      "Epoch 53/120\n",
      "10381/10381 [==============================] - 9s 837us/step - loss: 7.7170 - mean_absolute_error: 1.5622 3s - loss: 7.5536 - mean_absolute_error: 1.541 \n",
      "Epoch 54/120\n",
      "10381/10381 [==============================] - 8s 765us/step - loss: 7.6695 - mean_absolute_error: 1.5358\n",
      "Epoch 55/120\n",
      "10381/10381 [==============================] - 8s 802us/step - loss: 7.6825 - mean_absolute_error: 1.5446\n",
      "Epoch 56/120\n",
      "10381/10381 [==============================] - 8s 800us/step - loss: 7.7847 - mean_absolute_error: 1.5480\n",
      "Epoch 57/120\n",
      "10381/10381 [==============================] - 9s 870us/step - loss: 7.2836 - mean_absolute_error: 1.5082\n",
      "Epoch 58/120\n",
      "10381/10381 [==============================] - 8s 813us/step - loss: 7.3032 - mean_absolute_error: 1.5167\n",
      "Epoch 59/120\n",
      "10381/10381 [==============================] - 9s 820us/step - loss: 7.4903 - mean_absolute_error: 1.5201\n",
      "Epoch 60/120\n",
      "10381/10381 [==============================] - 9s 857us/step - loss: 7.2759 - mean_absolute_error: 1.5142\n",
      "Epoch 61/120\n",
      "10381/10381 [==============================] - 8s 819us/step - loss: 7.3141 - mean_absolute_error: 1.5058\n",
      "Epoch 62/120\n",
      "10381/10381 [==============================] - 9s 834us/step - loss: 7.5150 - mean_absolute_error: 1.5188\n",
      "Epoch 63/120\n",
      "10381/10381 [==============================] - 9s 829us/step - loss: 7.1091 - mean_absolute_error: 1.4843\n",
      "Epoch 64/120\n",
      "10381/10381 [==============================] - 9s 848us/step - loss: 7.5216 - mean_absolute_error: 1.5196\n",
      "Epoch 65/120\n",
      "10381/10381 [==============================] - 8s 795us/step - loss: 7.8092 - mean_absolute_error: 1.5127\n",
      "Epoch 66/120\n",
      "10381/10381 [==============================] - 9s 855us/step - loss: 7.5080 - mean_absolute_error: 1.5153 4s - loss: 7.6 - ETA: 2s - loss: 7.5755 - mean_absolute_error: - ETA: 2s - \n",
      "Epoch 67/120\n",
      "10381/10381 [==============================] - 9s 846us/step - loss: 7.5031 - mean_absolute_error: 1.5014\n",
      "Epoch 68/120\n",
      "10381/10381 [==============================] - 9s 852us/step - loss: 7.3961 - mean_absolute_error: 1.4968\n",
      "Epoch 69/120\n",
      "10381/10381 [==============================] - 9s 832us/step - loss: 7.0177 - mean_absolute_error: 1.4768\n",
      "Epoch 70/120\n",
      "10381/10381 [==============================] - 9s 877us/step - loss: 7.2526 - mean_absolute_error: 1.4861\n",
      "Epoch 71/120\n",
      "10381/10381 [==============================] - 9s 834us/step - loss: 7.2595 - mean_absolute_error: 1.4788\n",
      "Epoch 72/120\n",
      "10381/10381 [==============================] - 9s 823us/step - loss: 7.2454 - mean_absolute_error: 1.4841\n",
      "Epoch 73/120\n",
      "10381/10381 [==============================] - 9s 845us/step - loss: 7.0946 - mean_absolute_error: 1.4815 2s - loss: 7.\n",
      "Epoch 74/120\n",
      "10381/10381 [==============================] - 9s 851us/step - loss: 7.0800 - mean_absolute_error: 1.4736\n",
      "Epoch 75/120\n",
      "10381/10381 [==============================] - 9s 869us/step - loss: 7.3149 - mean_absolute_error: 1.4773 1s - loss: 7\n",
      "Epoch 76/120\n",
      "10381/10381 [==============================] - 9s 826us/step - loss: 7.1525 - mean_absolute_error: 1.4661\n",
      "Epoch 77/120\n",
      "10381/10381 [==============================] - 9s 885us/step - loss: 7.0797 - mean_absolute_error: 1.4602 1s - loss: 7.1355 \n",
      "Epoch 78/120\n",
      "10381/10381 [==============================] - 8s 800us/step - loss: 7.0851 - mean_absolute_error: 1.4721\n",
      "Epoch 79/120\n",
      "10381/10381 [==============================] - 9s 840us/step - loss: 6.9335 - mean_absolute_error: 1.4618\n",
      "Epoch 80/120\n",
      "10381/10381 [==============================] - 9s 829us/step - loss: 7.4612 - mean_absolute_error: 1.4889\n",
      "Epoch 81/120\n",
      "10381/10381 [==============================] - 9s 872us/step - loss: 7.1964 - mean_absolute_error: 1.4746\n",
      "Epoch 82/120\n",
      "10381/10381 [==============================] - 9s 821us/step - loss: 6.9968 - mean_absolute_error: 1.4535 1s - loss: 6.7782 - me\n",
      "Epoch 83/120\n",
      "10381/10381 [==============================] - 8s 811us/step - loss: 7.2860 - mean_absolute_error: 1.4887\n",
      "Epoch 84/120\n",
      "10381/10381 [==============================] - 9s 883us/step - loss: 6.9550 - mean_absolute_error: 1.4396 4s - loss: 6.8646 - mean_\n",
      "Epoch 85/120\n",
      "10381/10381 [==============================] - 8s 813us/step - loss: 7.2511 - mean_absolute_error: 1.4643\n",
      "Epoch 86/120\n",
      "10381/10381 [==============================] - 9s 840us/step - loss: 7.3237 - mean_absolute_error: 1.4778\n",
      "Epoch 87/120\n",
      "10381/10381 [==============================] - 9s 867us/step - loss: 6.7797 - mean_absolute_error: 1.4372 0s - loss: 6.7932 - mean_absolute_error: 1.4\n",
      "Epoch 88/120\n",
      "10381/10381 [==============================] - 11s 1ms/step - loss: 6.9157 - mean_absolute_error: 1.4378: 2s - l\n",
      "Epoch 89/120\n",
      "10381/10381 [==============================] - 10s 932us/step - loss: 6.7298 - mean_absolute_error: 1.4500\n",
      "Epoch 90/120\n",
      "10381/10381 [==============================] - 10s 928us/step - loss: 6.7695 - mean_absolute_error: 1.43440s - loss: 6.8235 - mean_absolute_e\n",
      "Epoch 91/120\n",
      "10381/10381 [==============================] - 10s 918us/step - loss: 6.7617 - mean_absolute_error: 1.4371\n",
      "Epoch 92/120\n",
      "10381/10381 [==============================] - 9s 841us/step - loss: 6.8326 - mean_absolute_error: 1.4347\n",
      "Epoch 93/120\n",
      "10381/10381 [==============================] - 8s 816us/step - loss: 7.4107 - mean_absolute_error: 1.4752\n",
      "Epoch 94/120\n",
      "10381/10381 [==============================] - 8s 798us/step - loss: 6.8177 - mean_absolute_error: 1.4391\n",
      "Epoch 95/120\n",
      "10381/10381 [==============================] - 8s 730us/step - loss: 6.8058 - mean_absolute_error: 1.4367\n",
      "Epoch 96/120\n",
      "10381/10381 [==============================] - 7s 687us/step - loss: 6.9772 - mean_absolute_error: 1.4330\n",
      "Epoch 97/120\n",
      "10381/10381 [==============================] - 8s 725us/step - loss: 6.9738 - mean_absolute_error: 1.4488\n",
      "Epoch 98/120\n",
      "10381/10381 [==============================] - 8s 807us/step - loss: 6.8978 - mean_absolute_error: 1.4350\n",
      "Epoch 99/120\n",
      "10381/10381 [==============================] - 8s 819us/step - loss: 6.7892 - mean_absolute_error: 1.4213\n",
      "Epoch 100/120\n",
      "10381/10381 [==============================] - 10s 945us/step - loss: 6.5770 - mean_absolute_error: 1.42852s - \n",
      "Epoch 101/120\n",
      "10381/10381 [==============================] - 10s 974us/step - loss: 6.7547 - mean_absolute_error: 1.43668s - loss: 6.1937 - mean_absolu - ETA: 7s - loss: 6.2134 - mean_a - ETA: 5s - loss: 6.4032 - mean_absol - ETA: 4s - loss: 6.3787 - mean_absolute_error: 1.4 - ETA: 4s - loss: 6.4410 - mean_abso - ETA: 3s - loss: 6.5675 - mean_absolute_err - ETA: 2s - loss: 6.6709 - mean_absolute_error:  - ETA: 2s - loss: 6.6539 - mean_absolute_er - ETA: 1s - loss: 6.7514 - mean_abso - ETA: 0s - loss: 6.7863 - mean_absolute_error\n",
      "Epoch 102/120\n",
      "10381/10381 [==============================] - 9s 907us/step - loss: 6.9646 - mean_absolute_error: 1.4345\n",
      "Epoch 103/120\n",
      "10381/10381 [==============================] - 9s 844us/step - loss: 6.9590 - mean_absolute_error: 1.4394\n",
      "Epoch 104/120\n",
      "10381/10381 [==============================] - 9s 892us/step - loss: 7.0393 - mean_absolute_error: 1.4538 2s - loss: 6.4986 - mean_ab - ETA: 1s - loss: 6.7971 - me\n",
      "Epoch 105/120\n",
      "10381/10381 [==============================] - 8s 805us/step - loss: 6.6790 - mean_absolute_error: 1.4166\n",
      "Epoch 106/120\n",
      "10381/10381 [==============================] - 9s 841us/step - loss: 6.9349 - mean_absolute_error: 1.4461\n",
      "Epoch 107/120\n",
      "10381/10381 [==============================] - 9s 821us/step - loss: 6.8984 - mean_absolute_error: 1.4453\n",
      "Epoch 108/120\n",
      "10381/10381 [==============================] - 9s 879us/step - loss: 6.5365 - mean_absolute_error: 1.4193\n",
      "Epoch 109/120\n",
      "10381/10381 [==============================] - 8s 799us/step - loss: 6.6278 - mean_absolute_error: 1.4187 0s - loss: 6.4741 - mean_abso\n",
      "Epoch 110/120\n",
      "10381/10381 [==============================] - 9s 830us/step - loss: 6.8984 - mean_absolute_error: 1.4239\n",
      "Epoch 111/120\n",
      "10381/10381 [==============================] - 9s 846us/step - loss: 6.5537 - mean_absolute_error: 1.4212\n",
      "Epoch 112/120\n",
      "10381/10381 [==============================] - 8s 791us/step - loss: 6.5805 - mean_absolute_error: 1.4135 0s - loss: 6.6218 - mean_absolute_error: \n",
      "Epoch 113/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10381/10381 [==============================] - 8s 797us/step - loss: 6.5160 - mean_absolute_error: 1.4077\n",
      "Epoch 114/120\n",
      "10381/10381 [==============================] - 8s 783us/step - loss: 6.6576 - mean_absolute_error: 1.4215\n",
      "Epoch 115/120\n",
      "10381/10381 [==============================] - 9s 830us/step - loss: 6.6325 - mean_absolute_error: 1.4137\n",
      "Epoch 116/120\n",
      "10381/10381 [==============================] - 8s 804us/step - loss: 6.8620 - mean_absolute_error: 1.4294\n",
      "Epoch 117/120\n",
      "10381/10381 [==============================] - 9s 823us/step - loss: 6.9705 - mean_absolute_error: 1.4471\n",
      "Epoch 118/120\n",
      "10381/10381 [==============================] - 8s 810us/step - loss: 6.7003 - mean_absolute_error: 1.4177 2s - l\n",
      "Epoch 119/120\n",
      "10381/10381 [==============================] - 8s 819us/step - loss: 6.5456 - mean_absolute_error: 1.4026\n",
      "Epoch 120/120\n",
      "10381/10381 [==============================] - 8s 779us/step - loss: 6.5239 - mean_absolute_error: 1.4135\n",
      "Kappa Score: 0.9607685864369392\n",
      "\n",
      "--------Fold 5--------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/120\n",
      "10381/10381 [==============================] - 13s 1ms/step - loss: 62.7336 - mean_absolute_error: 4.3501\n",
      "Epoch 2/120\n",
      "10381/10381 [==============================] - 9s 866us/step - loss: 39.2098 - mean_absolute_error: 3.5211\n",
      "Epoch 3/120\n",
      "10381/10381 [==============================] - 9s 846us/step - loss: 34.1346 - mean_absolute_error: 3.4738\n",
      "Epoch 4/120\n",
      "10381/10381 [==============================] - 9s 845us/step - loss: 31.2731 - mean_absolute_error: 3.4026\n",
      "Epoch 5/120\n",
      "10381/10381 [==============================] - 9s 909us/step - loss: 29.2559 - mean_absolute_error: 3.3707\n",
      "Epoch 6/120\n",
      "10381/10381 [==============================] - 9s 832us/step - loss: 29.0925 - mean_absolute_error: 3.3301\n",
      "Epoch 7/120\n",
      "10381/10381 [==============================] - 9s 882us/step - loss: 27.4250 - mean_absolute_error: 3.1678\n",
      "Epoch 8/120\n",
      "10381/10381 [==============================] - 9s 862us/step - loss: 25.9265 - mean_absolute_error: 2.9978\n",
      "Epoch 9/120\n",
      "10381/10381 [==============================] - 9s 840us/step - loss: 22.9875 - mean_absolute_error: 2.8205\n",
      "Epoch 10/120\n",
      "10381/10381 [==============================] - 9s 837us/step - loss: 20.3377 - mean_absolute_error: 2.6502\n",
      "Epoch 11/120\n",
      "10381/10381 [==============================] - 9s 835us/step - loss: 18.5136 - mean_absolute_error: 2.4734\n",
      "Epoch 12/120\n",
      "10381/10381 [==============================] - 9s 874us/step - loss: 17.2520 - mean_absolute_error: 2.4038\n",
      "Epoch 13/120\n",
      "10381/10381 [==============================] - 9s 828us/step - loss: 16.2757 - mean_absolute_error: 2.3197\n",
      "Epoch 14/120\n",
      "10381/10381 [==============================] - 9s 871us/step - loss: 14.7969 - mean_absolute_error: 2.2088\n",
      "Epoch 15/120\n",
      "10381/10381 [==============================] - 9s 851us/step - loss: 14.3148 - mean_absolute_error: 2.1649\n",
      "Epoch 16/120\n",
      "10381/10381 [==============================] - 9s 849us/step - loss: 13.7632 - mean_absolute_error: 2.12360s - loss: 13.6811 - mean_absolute_error: \n",
      "Epoch 17/120\n",
      "10381/10381 [==============================] - 8s 806us/step - loss: 13.0604 - mean_absolute_error: 2.0776\n",
      "Epoch 18/120\n",
      "10381/10381 [==============================] - 9s 869us/step - loss: 12.4017 - mean_absolute_error: 2.0471\n",
      "Epoch 19/120\n",
      "10381/10381 [==============================] - 9s 843us/step - loss: 12.5169 - mean_absolute_error: 2.0336\n",
      "Epoch 20/120\n",
      "10381/10381 [==============================] - 9s 834us/step - loss: 12.1447 - mean_absolute_error: 1.9873\n",
      "Epoch 21/120\n",
      "10381/10381 [==============================] - 9s 842us/step - loss: 11.5495 - mean_absolute_error: 1.9491\n",
      "Epoch 22/120\n",
      "10381/10381 [==============================] - 9s 857us/step - loss: 11.3427 - mean_absolute_error: 1.9076\n",
      "Epoch 23/120\n",
      "10381/10381 [==============================] - 9s 849us/step - loss: 11.3802 - mean_absolute_error: 1.9143\n",
      "Epoch 24/120\n",
      "10381/10381 [==============================] - 8s 808us/step - loss: 10.8819 - mean_absolute_error: 1.8716\n",
      "Epoch 25/120\n",
      "10381/10381 [==============================] - 10s 972us/step - loss: 10.4260 - mean_absolute_error: 1.8438s -\n",
      "Epoch 26/120\n",
      "10381/10381 [==============================] - 9s 850us/step - loss: 10.0186 - mean_absolute_error: 1.8063\n",
      "Epoch 27/120\n",
      "10381/10381 [==============================] - 9s 847us/step - loss: 10.0260 - mean_absolute_error: 1.7985\n",
      "Epoch 28/120\n",
      "10381/10381 [==============================] - 9s 826us/step - loss: 10.0362 - mean_absolute_error: 1.7868\n",
      "Epoch 29/120\n",
      "10381/10381 [==============================] - 9s 843us/step - loss: 9.9453 - mean_absolute_error: 1.7880\n",
      "Epoch 30/120\n",
      "10381/10381 [==============================] - 9s 821us/step - loss: 9.7199 - mean_absolute_error: 1.7643\n",
      "Epoch 31/120\n",
      "10381/10381 [==============================] - 8s 802us/step - loss: 9.6789 - mean_absolute_error: 1.7625\n",
      "Epoch 32/120\n",
      "10381/10381 [==============================] - 9s 870us/step - loss: 9.5291 - mean_absolute_error: 1.7285 2s - loss: 9.2296 - me - ETA: 0s - loss: 9.4055 - mean_absolute\n",
      "Epoch 33/120\n",
      "10381/10381 [==============================] - 8s 791us/step - loss: 9.6585 - mean_absolute_error: 1.7408\n",
      "Epoch 34/120\n",
      "10381/10381 [==============================] - 9s 831us/step - loss: 10.1187 - mean_absolute_error: 1.7546\n",
      "Epoch 35/120\n",
      "10381/10381 [==============================] - 8s 811us/step - loss: 9.6199 - mean_absolute_error: 1.7204\n",
      "Epoch 36/120\n",
      "10381/10381 [==============================] - 9s 867us/step - loss: 9.1033 - mean_absolute_error: 1.6864\n",
      "Epoch 37/120\n",
      "10381/10381 [==============================] - 8s 792us/step - loss: 9.1730 - mean_absolute_error: 1.6865\n",
      "Epoch 38/120\n",
      "10381/10381 [==============================] - 9s 846us/step - loss: 8.9058 - mean_absolute_error: 1.6790\n",
      "Epoch 39/120\n",
      "10381/10381 [==============================] - 9s 834us/step - loss: 8.9288 - mean_absolute_error: 1.6658\n",
      "Epoch 40/120\n",
      "10381/10381 [==============================] - 9s 834us/step - loss: 8.7894 - mean_absolute_error: 1.6475\n",
      "Epoch 41/120\n",
      "10381/10381 [==============================] - 9s 828us/step - loss: 9.0186 - mean_absolute_error: 1.6722\n",
      "Epoch 42/120\n",
      "10381/10381 [==============================] - 9s 845us/step - loss: 8.9941 - mean_absolute_error: 1.6712 0s - loss: 9.0099 - mean_absolute_error: 1.67 - ETA: 0s - loss: 9.0085 - mean_absolute_error: 1.6\n",
      "Epoch 43/120\n",
      "10381/10381 [==============================] - 9s 846us/step - loss: 8.8227 - mean_absolute_error: 1.6541\n",
      "Epoch 44/120\n",
      "10381/10381 [==============================] - 8s 808us/step - loss: 8.7708 - mean_absolute_error: 1.6318\n",
      "Epoch 45/120\n",
      "10381/10381 [==============================] - 9s 836us/step - loss: 8.7094 - mean_absolute_error: 1.6387\n",
      "Epoch 46/120\n",
      "10381/10381 [==============================] - 9s 828us/step - loss: 8.6780 - mean_absolute_error: 1.6253\n",
      "Epoch 47/120\n",
      "10381/10381 [==============================] - 9s 855us/step - loss: 8.6164 - mean_absolute_error: 1.6089\n",
      "Epoch 48/120\n",
      "10381/10381 [==============================] - ETA: 0s - loss: 8.4587 - mean_absolute_error: 1.608 - 8s 792us/step - loss: 8.4555 - mean_absolute_error: 1.6089\n",
      "Epoch 49/120\n",
      "10381/10381 [==============================] - 9s 835us/step - loss: 8.9316 - mean_absolute_error: 1.6313 2s - loss: 9.\n",
      "Epoch 50/120\n",
      "10381/10381 [==============================] - 8s 779us/step - loss: 9.0573 - mean_absolute_error: 1.6251\n",
      "Epoch 51/120\n",
      "10381/10381 [==============================] - 8s 803us/step - loss: 8.1760 - mean_absolute_error: 1.5937\n",
      "Epoch 52/120\n",
      "10381/10381 [==============================] - 8s 775us/step - loss: 8.4680 - mean_absolute_error: 1.6097\n",
      "Epoch 53/120\n",
      "10381/10381 [==============================] - 9s 835us/step - loss: 8.2739 - mean_absolute_error: 1.5854\n",
      "Epoch 54/120\n",
      "10381/10381 [==============================] - 8s 772us/step - loss: 8.6987 - mean_absolute_error: 1.6099\n",
      "Epoch 55/120\n",
      "10381/10381 [==============================] - 8s 809us/step - loss: 8.3422 - mean_absolute_error: 1.5835\n",
      "Epoch 56/120\n",
      "10381/10381 [==============================] - 8s 783us/step - loss: 8.1353 - mean_absolute_error: 1.5925\n",
      "Epoch 57/120\n",
      "10381/10381 [==============================] - 9s 842us/step - loss: 8.4314 - mean_absolute_error: 1.5929\n",
      "Epoch 58/120\n",
      "10381/10381 [==============================] - 8s 779us/step - loss: 8.6549 - mean_absolute_error: 1.6076\n",
      "Epoch 59/120\n",
      "10381/10381 [==============================] - 8s 798us/step - loss: 8.5170 - mean_absolute_error: 1.5816\n",
      "Epoch 60/120\n",
      "10381/10381 [==============================] - 9s 821us/step - loss: 8.2929 - mean_absolute_error: 1.5656\n",
      "Epoch 61/120\n",
      "10381/10381 [==============================] - 8s 804us/step - loss: 8.2358 - mean_absolute_error: 1.5872\n",
      "Epoch 62/120\n",
      "10381/10381 [==============================] - 8s 796us/step - loss: 7.8961 - mean_absolute_error: 1.5599\n",
      "Epoch 63/120\n",
      "10381/10381 [==============================] - 8s 794us/step - loss: 7.9884 - mean_absolute_error: 1.5485\n",
      "Epoch 64/120\n",
      "10381/10381 [==============================] - 9s 862us/step - loss: 7.8574 - mean_absolute_error: 1.5649\n",
      "Epoch 65/120\n",
      "10381/10381 [==============================] - 8s 790us/step - loss: 8.1966 - mean_absolute_error: 1.5733\n",
      "Epoch 66/120\n",
      "10381/10381 [==============================] - 8s 813us/step - loss: 8.1574 - mean_absolute_error: 1.5661\n",
      "Epoch 67/120\n",
      "10381/10381 [==============================] - 8s 804us/step - loss: 7.8892 - mean_absolute_error: 1.5416\n",
      "Epoch 68/120\n",
      "10381/10381 [==============================] - 9s 880us/step - loss: 7.9054 - mean_absolute_error: 1.5434\n",
      "Epoch 69/120\n",
      "10381/10381 [==============================] - 8s 791us/step - loss: 7.6852 - mean_absolute_error: 1.5351\n",
      "Epoch 70/120\n",
      "10381/10381 [==============================] - 9s 827us/step - loss: 7.9288 - mean_absolute_error: 1.5511\n",
      "Epoch 71/120\n",
      "10381/10381 [==============================] - 9s 827us/step - loss: 7.7376 - mean_absolute_error: 1.5393\n",
      "Epoch 72/120\n",
      "10381/10381 [==============================] - 9s 832us/step - loss: 7.8957 - mean_absolute_error: 1.5495\n",
      "Epoch 73/120\n",
      "10381/10381 [==============================] - 8s 802us/step - loss: 7.7489 - mean_absolute_error: 1.5185\n",
      "Epoch 74/120\n",
      "10381/10381 [==============================] - 9s 870us/step - loss: 7.3052 - mean_absolute_error: 1.4991\n",
      "Epoch 75/120\n",
      "10381/10381 [==============================] - 9s 828us/step - loss: 7.7093 - mean_absolute_error: 1.5330\n",
      "Epoch 76/120\n",
      "10381/10381 [==============================] - 8s 816us/step - loss: 7.2284 - mean_absolute_error: 1.5130\n",
      "Epoch 77/120\n",
      "10381/10381 [==============================] - 9s 836us/step - loss: 7.8813 - mean_absolute_error: 1.5506 2s\n",
      "Epoch 78/120\n",
      "10381/10381 [==============================] - 9s 850us/step - loss: 7.6282 - mean_absolute_error: 1.5106\n",
      "Epoch 79/120\n",
      "10381/10381 [==============================] - 9s 848us/step - loss: 7.8215 - mean_absolute_error: 1.5204 3s - loss: 7.5590 - mean_absolute_er - ETA: \n",
      "Epoch 80/120\n",
      "10381/10381 [==============================] - 8s 815us/step - loss: 7.7061 - mean_absolute_error: 1.5186\n",
      "Epoch 81/120\n",
      "10381/10381 [==============================] - 9s 911us/step - loss: 7.9689 - mean_absolute_error: 1.5474 0s - loss: 7.9416 - mean_absolute_erro\n",
      "Epoch 82/120\n",
      "10381/10381 [==============================] - 8s 816us/step - loss: 7.5620 - mean_absolute_error: 1.5013\n",
      "Epoch 83/120\n",
      "10381/10381 [==============================] - 9s 840us/step - loss: 7.4673 - mean_absolute_error: 1.5183\n",
      "Epoch 84/120\n",
      "10381/10381 [==============================] - 8s 812us/step - loss: 7.9227 - mean_absolute_error: 1.5252\n",
      "Epoch 85/120\n",
      "10381/10381 [==============================] - 9s 901us/step - loss: 7.6400 - mean_absolute_error: 1.5065\n",
      "Epoch 86/120\n",
      "10381/10381 [==============================] - 9s 820us/step - loss: 7.4142 - mean_absolute_error: 1.4969\n",
      "Epoch 87/120\n",
      "10381/10381 [==============================] - 8s 816us/step - loss: 7.6904 - mean_absolute_error: 1.5033\n",
      "Epoch 88/120\n",
      "10381/10381 [==============================] - 9s 863us/step - loss: 7.6660 - mean_absolute_error: 1.5128 3s - loss: 7.5400 - m - ETA: 1s - loss: 7.7254 - mean_absolute_error: 1.51 - ETA: 1s - loss: 7.7062 - mean_absolute_error: 1.51 - ETA: 1s - loss: 7.6805 - \n",
      "Epoch 89/120\n",
      "10381/10381 [==============================] - 9s 848us/step - loss: 7.1609 - mean_absolute_error: 1.4927\n",
      "Epoch 90/120\n",
      "10381/10381 [==============================] - 8s 811us/step - loss: 7.5746 - mean_absolute_error: 1.4990 0s - loss: 7.5910 - mean_absolute_erro\n",
      "Epoch 91/120\n",
      "10381/10381 [==============================] - 8s 780us/step - loss: 7.3717 - mean_absolute_error: 1.4837 0s - loss: 7.4071 - mean_absolute_er\n",
      "Epoch 92/120\n",
      "10381/10381 [==============================] - 9s 847us/step - loss: 7.2147 - mean_absolute_error: 1.4907\n",
      "Epoch 93/120\n",
      "10381/10381 [==============================] - 8s 809us/step - loss: 7.5205 - mean_absolute_error: 1.4788\n",
      "Epoch 94/120\n",
      "10381/10381 [==============================] - 9s 849us/step - loss: 7.7083 - mean_absolute_error: 1.5286\n",
      "Epoch 95/120\n",
      "10381/10381 [==============================] - 9s 830us/step - loss: 7.6188 - mean_absolute_error: 1.5025 2s - loss: \n",
      "Epoch 96/120\n",
      "10381/10381 [==============================] - 9s 847us/step - loss: 7.3467 - mean_absolute_error: 1.4709\n",
      "Epoch 97/120\n",
      "10381/10381 [==============================] - 8s 785us/step - loss: 7.0969 - mean_absolute_error: 1.4796 4s - loss: 7.0100 - mean_absolute_error: 1.4 - ETA: 0s - loss: 7.1112 - mean_absolute_er\n",
      "Epoch 98/120\n",
      "10381/10381 [==============================] - 9s 825us/step - loss: 7.1871 - mean_absolute_error: 1.4811\n",
      "Epoch 99/120\n",
      "10381/10381 [==============================] - 9s 840us/step - loss: 7.2703 - mean_absolute_error: 1.4828\n",
      "Epoch 100/120\n",
      "10381/10381 [==============================] - 8s 809us/step - loss: 7.2500 - mean_absolute_error: 1.4838\n",
      "Epoch 101/120\n",
      "10381/10381 [==============================] - 9s 865us/step - loss: 7.3454 - mean_absolute_error: 1.4950\n",
      "Epoch 102/120\n",
      "10381/10381 [==============================] - 9s 828us/step - loss: 7.4440 - mean_absolute_error: 1.4788\n",
      "Epoch 103/120\n",
      "10381/10381 [==============================] - 9s 823us/step - loss: 7.2272 - mean_absolute_error: 1.4760\n",
      "Epoch 104/120\n",
      "10381/10381 [==============================] - 8s 804us/step - loss: 7.5720 - mean_absolute_error: 1.5062\n",
      "Epoch 105/120\n",
      "10381/10381 [==============================] - 9s 857us/step - loss: 7.1694 - mean_absolute_error: 1.4661\n",
      "Epoch 106/120\n",
      "10381/10381 [==============================] - 8s 813us/step - loss: 7.1035 - mean_absolute_error: 1.4802\n",
      "Epoch 107/120\n",
      "10381/10381 [==============================] - 9s 822us/step - loss: 7.1139 - mean_absolute_error: 1.4708\n",
      "Epoch 108/120\n",
      "10381/10381 [==============================] - 8s 782us/step - loss: 7.2767 - mean_absolute_error: 1.4681\n",
      "Epoch 109/120\n",
      "10381/10381 [==============================] - 9s 853us/step - loss: 6.8456 - mean_absolute_error: 1.4500\n",
      "Epoch 110/120\n",
      "10381/10381 [==============================] - 8s 785us/step - loss: 6.9640 - mean_absolute_error: 1.4660 2s -\n",
      "Epoch 111/120\n",
      "10381/10381 [==============================] - 9s 835us/step - loss: 7.3973 - mean_absolute_error: 1.4704\n",
      "Epoch 112/120\n",
      "10381/10381 [==============================] - 8s 809us/step - loss: 6.7092 - mean_absolute_error: 1.4497\n",
      "Epoch 113/120\n",
      "10381/10381 [==============================] - 9s 862us/step - loss: 7.2998 - mean_absolute_error: 1.4733\n",
      "Epoch 114/120\n",
      "10381/10381 [==============================] - 8s 791us/step - loss: 7.0623 - mean_absolute_error: 1.4594\n",
      "Epoch 115/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10381/10381 [==============================] - 8s 791us/step - loss: 7.2001 - mean_absolute_error: 1.4868\n",
      "Epoch 116/120\n",
      "10381/10381 [==============================] - 9s 833us/step - loss: 7.3628 - mean_absolute_error: 1.4751\n",
      "Epoch 117/120\n",
      "10381/10381 [==============================] - 8s 798us/step - loss: 6.9710 - mean_absolute_error: 1.4515\n",
      "Epoch 118/120\n",
      "10381/10381 [==============================] - 8s 795us/step - loss: 7.2170 - mean_absolute_error: 1.4697\n",
      "Epoch 119/120\n",
      "10381/10381 [==============================] - 8s 798us/step - loss: 6.9644 - mean_absolute_error: 1.4451\n",
      "Epoch 120/120\n",
      "10381/10381 [==============================] - 9s 835us/step - loss: 7.2723 - mean_absolute_error: 1.4719\n",
      "Kappa Score: 0.9602836463725429\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "cv = KFold(len(X), n_folds=5, shuffle=True)\n",
    "results = []\n",
    "y_pred_list = []\n",
    "\n",
    "count = 1\n",
    "for traincv, testcv in cv:\n",
    "    print(\"\\n--------Fold {}--------\\n\".format(count))\n",
    "    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
    "    \n",
    "    train_essays = X_train['essay']\n",
    "    test_essays = X_test['essay']\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for essay in train_essays:\n",
    "            # Obtaining all sentences from the training essays.\n",
    "            sentences += essay_to_sentences(essay, remove_stopwords = True)\n",
    "            \n",
    "    # Initializing variables for word2vec model.\n",
    "    num_features = 300 \n",
    "    min_word_count = 40\n",
    "    num_workers = 4\n",
    "    context = 10\n",
    "    downsampling = 1e-3\n",
    "\n",
    "    print(\"Training Word2Vec Model...\")\n",
    "    model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
    "\n",
    "    model.init_sims(replace=True)\n",
    "    model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n",
    "\n",
    "    clean_train_essays = []\n",
    "    \n",
    "    # Generate training and testing data word vectors.\n",
    "    for essay_v in train_essays:\n",
    "        clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
    "    trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n",
    "    \n",
    "    clean_test_essays = []\n",
    "    for essay_v in test_essays:\n",
    "        clean_test_essays.append(essay_to_wordlist( essay_v, remove_stopwords=True ))\n",
    "    testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
    "    \n",
    "    trainDataVecs = np.array(trainDataVecs)\n",
    "    testDataVecs = np.array(testDataVecs)\n",
    "    # Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n",
    "    trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
    "    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
    "    \n",
    "    lstm_model = get_model()\n",
    "    history = lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=120)\n",
    "    #lstm_model.load_weights('./model_weights/final_lstm.h5')\n",
    "    y_pred = lstm_model.predict(testDataVecs)\n",
    "    \n",
    "    # Save any one of the 8 models.\n",
    "    if count == 5:\n",
    "         lstm_model.save('./model_weights/final_lstm.h5')\n",
    "    \n",
    "    # Round y_pred to the nearest integer.\n",
    "    y_pred = np.around(y_pred)\n",
    "    \n",
    "    # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
    "    result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
    "    print(\"Kappa Score: {}\".format(result))\n",
    "    results.append(result)\n",
    "\n",
    "    count += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Kappa score after a 5-fold cross validation:  0.9639\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results).mean(),decimals=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9620323017556787,\n",
       " 0.9671342305342847,\n",
       " 0.9694664048286248,\n",
       " 0.9607685864369392,\n",
       " 0.9602836463725429]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "lis = [0.9639, 0.9345, 0.9311]\n",
    "bars = ('K-fold', '70:30', '80:20')\n",
    "y_pos = np.arange(len(bars))\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"models\")\n",
    "plt.bar(bars,lis, width=0.4, alpha=0.5, color=['r','g','b','y'])\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
